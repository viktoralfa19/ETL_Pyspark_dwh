{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "58eaaeb8-624c-42e0-bc3b-6cc7c47f47c0",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Transformación de tipo pandas a pyspark\n",
    "\n",
    "class Transformacion:\n",
    "    \"\"\"Clase para realizar la transformación de dataframe pandas a dataframe pyspark\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def equivalent_type(f):\n",
    "        if f == 'datetime64[ns]': return DateType()\n",
    "        elif f == 'int64': return LongType()\n",
    "        elif f == 'int32': return IntegerType()\n",
    "        elif f == 'float64': return FloatType()\n",
    "        else: return StringType()\n",
    "\n",
    "    def define_structure(self, string, format_type):\n",
    "        try: typo = self.equivalent_type(format_type)\n",
    "        except: typo = StringType()\n",
    "        return StructField(string, typo)\n",
    "\n",
    "\n",
    "    def pandas_to_spark(self, pandas_df, spark):\n",
    "        columns = list(pandas_df.columns)\n",
    "        types = list(pandas_df.dtypes)\n",
    "        struct_list = []\n",
    "        for column, typo in zip(columns, types): \n",
    "            struct_list.append(self.define_structure(column, typo))\n",
    "        p_schema = StructType(struct_list)\n",
    "        return spark.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "6224da63-f576-48b0-9fb9-b8d57cd03d3b",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Utilitarios\n",
    "\n",
    "class Utilitarios:\n",
    "    \"\"\"Clase para usar métodos bastante genéricos y en muchos casos estáticos\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def GenerateDataFileName(catalogo, fecha):\n",
    "        return 'file_'+catalogo+'_'+fecha\n",
    "    \n",
    "    @staticmethod\n",
    "    def GenerateCatalogueFileName(catalogo):\n",
    "        return \"file_\"+catalogo\n",
    "    \n",
    "    @staticmethod\n",
    "    def ConvertPandasToSpark(spark, dataframe_pandas, schema):\n",
    "        try:\n",
    "            if(dataframe_pandas is None):\n",
    "                return spark.createDataFrame([], schema)\n",
    "            else:\n",
    "                return spark.createDataFrame(dataframe_pandas, schema)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertSparkToPandas(dataframe_spark):\n",
    "        try:\n",
    "            if(dataframe_spark is None):\n",
    "                return None\n",
    "            return dataframe_spark.toPandas()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertStrListToSpark(spark, pd, str_list, list_column_names):\n",
    "        try:\n",
    "            if(str_list is None or len(str_list) == 0):\n",
    "                return None\n",
    "            return spark.createDataFrame((pd.DataFrame(str_list, columns = list_column_names)))\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertObjListToPandas(pd, obj_list, list_column_names):\n",
    "        try:\n",
    "            if(obj_list is None or len(obj_list) == 0):\n",
    "                return None\n",
    "            return pd.DataFrame(obj_list, columns = list_column_names)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "7fd3fd9a-7bc0-4283-ace1-7a76a7c848f0",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Manejo de Excepciones\n",
    "\n",
    "import sys, traceback\n",
    "\n",
    "class ExceptionManager:\n",
    "    \"\"\"Clase para manejar las excepciones.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Treatment(exception):\n",
    "        try:\n",
    "            print(exception)\n",
    "        except Py4JNetworkError as error:\n",
    "            print(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def TraceTreatment(exception):\n",
    "        try:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback)))\n",
    "            print(exception)\n",
    "        except Py4JNetworkError as error:\n",
    "            print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "33cf33aa-83ce-4f26-b0f4-ce76f309cdb5",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Configuracion de Contextos\n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "from sqlalchemy.orm import sessionmaker\n",
    "    \n",
    "class DBContextDw: \n",
    "        \"\"\"Clase que permite establecer la configuración con la bodega de datos o DataWarehouse.\"\"\"  \n",
    "                    \n",
    "        def __init__ (self):\n",
    "            self.HostDb= \"10.30.80.3\"\n",
    "            self.Port = \"5432\"\n",
    "            self.UserName = \"user_sirio\"\n",
    "            self.Password =\"Cen.2019.sirio\"\n",
    "            self.DataBase = \"dm_eventos\"\n",
    "            self.session_marker = None\n",
    "            \n",
    "        def Connection(self):\n",
    "            \"\"\"Método que permite obtener la sesión de conección a la base de datos\"\"\"\n",
    "            db_string = 'postgresql://{0}:{1}@{2}:{3}/{4}'.format(self.UserName, self.Password, self.HostDb,\n",
    "                                                                  self.Port, self.DataBase)\n",
    "            db = create_engine(db_string)  \n",
    "            self.session_maker = sessionmaker(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "azdata_cell_guid": "b8cf14e4-341f-484f-8a5f-fd9f6853825e",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Configuracion de Contextos\n",
    "\n",
    "class HDFSContext: \n",
    "    \"\"\"La clase ContextHdfs establece la configuración con el datalake para acceder a los diferentes archivos \n",
    "    almacenados de las bases de datos del HDFS.\"\"\"\n",
    "    \n",
    "    def __init__ (self, Host='10.30.80.3', Port='9000', Path='DATABASE/RDBMS', DataBase='SIVO', Schema='dbo'):\n",
    "        self.HostHdfs = Host\n",
    "        self.Port = Port\n",
    "        self.Path = Path\n",
    "        self.DataBase = DataBase\n",
    "        self.Schema = Schema\n",
    "        \n",
    "    def HdfsPath(self, tName, fName):        \n",
    "        \"\"\"Método que establece el path de búsqueda de un archivo específico.\"\"\"  \n",
    "        pathDir = \"hdfs://{0}:{1}/{2}/{3}/{4}/{5}/{6}\".format(self.HostHdfs, self.Port, self.Path, self.DataBase,\n",
    "                                                              self.Schema, tName, fName)\n",
    "        return pathDir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "azdata_cell_guid": "596cabc7-2e86-46ec-925c-9758f4d175b9",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Acceso a datos desde HDFS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class GenericDataFrame():\n",
    "    \"\"\"Clase para generar DataFrames Spark y trabajar en la lógica del ETL\"\"\"\n",
    "    def __init__ (self, hdfsContext):\n",
    "        self.hdfsContext = hdfsContext\n",
    "        self.url = 'jdbc:postgresql://10.30.80.3/dm_eventos'\n",
    "        self.properties = {'user': 'user_sirio', 'password': 'Cen.2019.sirio'}\n",
    "        self.modo = 'append'\n",
    "        self.spark = SparkSession.builder.appName(\"Sirio\")\\\n",
    "        .config('spark.driver.extraClassPath', '/home/jovyan/work/postgresql-42.2.12.jar').getOrCreate()#SparkSession.builder.appName(\"Sirio\").getOrCreate()\n",
    "    \n",
    "    def GetDataHdfs(self, tableName, fileName):\n",
    "        \"\"\"Método para retornar el DataFrame desde hadoop\"\"\"\n",
    "        path = self.hdfsContext.HdfsPath(tableName, fileName)\n",
    "        dataFrame = self.spark.read.json(path, multiLine=True)            \n",
    "        return dataFrame              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "azdata_cell_guid": "f494ff80-2c90-4d1c-9632-e9c9b670ffe6",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Creación del modelo\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base  \n",
    "from sqlalchemy import Column, String, SmallInteger, Integer, Numeric, BigInteger, DateTime, Boolean\n",
    "\n",
    "class Dim_Agt_Gen(declarative_base()):  \n",
    "    def __init__(self, pk, codigo_empresa, empresa, codigo_unegocio, unegocio, codigo_central, central\n",
    "                , codigo_unidad, unidad, fecha_operacion_comercial, version, pot_efectiva):\n",
    "        self.agtevt_id_pk = pk\n",
    "        self.agtevt_empresa_id_bk = codigo_empresa\n",
    "        self.agtevt_empresa = empresa\n",
    "        self.agtevt_unidad_negocio_id_bk = codigo_unegocio\n",
    "        self.agtevt_unidad_negocio = unegocio\n",
    "        self.agtevt_central_id_bk = codigo_central\n",
    "        self.agtevt_central = central\n",
    "        self.agtevt_unidad_id_bk = codigo_unidad\n",
    "        self.agtevt_unidad = unidad\n",
    "        self.agtevt_fecha_oper_comercial = fecha_operacion_comercial\n",
    "        self.agtevt_version = version\n",
    "        self.agtevt_pot_efectiva = pot_efectiva\n",
    "    \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['agtevt_id_pk', 'agtevt_empresa_id_bk', 'agtevt_empresa', 'agtevt_unidad_negocio_id_bk', 'agtevt_unidad_negocio'\n",
    "                , 'agtevt_central_id_bk', 'agtevt_central', 'agtevt_unidad_id_bk', 'agtevt_unidad', 'agtevt_fecha_oper_comercial'\n",
    "                , 'agtevt_version', 'agtevt_pot_efectiva']\n",
    "        \n",
    "    __tablename__ = 'dim_agt_gen'\n",
    "    __table_args__ = {'schema' : 'cen_dws'} \n",
    "\n",
    "    agtevt_id_pk = Column(Integer, primary_key=True)\n",
    "    agtevt_empresa_id_bk = Column(String)\n",
    "    agtevt_empresa = Column(String)\n",
    "    agtevt_unidad_negocio_id_bk = Column(String)\n",
    "    agtevt_unidad_negocio = Column(String)\n",
    "    agtevt_central_id_bk = Column(String)\n",
    "    agtevt_central = Column(String)\n",
    "    agtevt_unidad_id_bk = Column(String)\n",
    "    agtevt_unidad = Column(String)\n",
    "    agtevt_fecha_oper_comercial = Column(DateTime)\n",
    "    agtevt_version = Column(SmallInteger)\n",
    "    agtevt_pot_efectiva = Column(Numeric(10,2))\n",
    "    \n",
    "    \n",
    "class Dim_Tmp_Operacion(declarative_base()):  \n",
    "    def __init__(self, pk, fecha, anio, id_mes, mes, dia, hora):\n",
    "        self.tmpop_id_pk = pk\n",
    "        self.tmpop_fecha = fecha\n",
    "        self.tmpop_anio = anio\n",
    "        self.tmpop_mes_id = id_mes\n",
    "        self.tmpop_mes = mes\n",
    "        self.tmpop_dia = dia\n",
    "        self.tmpop_hora = hora\n",
    "      \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['tmpop_id_pk', 'tmpop_fecha', 'tmpop_anio', 'tmpop_mes_id', 'tmpop_mes', 'tmpop_dia', 'tmpop_hora']\n",
    "    \n",
    "    __tablename__ = 'dim_tmp_operacion'\n",
    "    __table_args__ = {'schema' : 'cen_dws'}\n",
    "\n",
    "    tmpop_id_pk = Column(Integer, primary_key=True)\n",
    "    tmpop_fecha = Column(DateTime)\n",
    "    tmpop_anio = Column(SmallInteger)    \n",
    "    tmpop_mes_id = Column(SmallInteger)\n",
    "    tmpop_mes = Column(String)\n",
    "    tmpop_dia = Column(SmallInteger)\n",
    "    tmpop_hora = Column(SmallInteger)\n",
    "      \n",
    "    \n",
    "class Fact_Pot_Disponible(declarative_base()): \n",
    "    \"\"\"Módelo de la tabla de hechos\"\"\"\n",
    "    def __init__(self, pk, agente_gen, tmpop, potencia_disponible):\n",
    "        self.potdisp_id_pk = pk\n",
    "        self.agtevt_id_fk = agente_gen\n",
    "        self.tmpop_id_fk = tmpop\n",
    "        self.potdisp_pot_dispon = potencia_disponible\n",
    "        \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['potdisp_id_pk', 'agtevt_id_fk', 'tmpop_id_fk', 'potdisp_pot_dispon']\n",
    "    \n",
    "    __tablename__ = 'fact_pot_disponible'\n",
    "    __table_args__ = {'schema' : 'cen_dws'}\n",
    "\n",
    "    potdisp_id_pk = Column(BigInteger, primary_key=True) \n",
    "    agtevt_id_fk = Column(Integer)\n",
    "    tmpop_id_fk = Column(Integer)\n",
    "    potdisp_pot_dispon = Column(Numeric(10, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "7cec5f13-2317-44ad-8811-5c05758d8d27",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Estructuras de Datos Esquemas\n",
    "\n",
    "class Estructuras:\n",
    "    \"\"\"Clase para obtener de manera estática las estructuras de los dataframes\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Agt_Gen():\n",
    "        schema = StructType([StructField('agtevt_id_pk', IntegerType(), False),\n",
    "                             StructField('agtevt_empresa_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_empresa', StringType(), False),\n",
    "                             StructField('agtevt_unidad_negocio_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_unidad_negocio', StringType(), False),\n",
    "                             StructField('agtevt_central_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_central', StringType(), False),\n",
    "                             StructField('agtevt_unidad_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_unidad', StringType(), False),\n",
    "                             StructField('agtevt_fecha_oper_comercial', TimestampType(), False),\n",
    "                             StructField('agtevt_version', ShortType(), False),\n",
    "                             StructField('agtevt_pot_efectiva', FloatType(), False)\n",
    "                             ])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Tmp_Operacion():\n",
    "        schema = StructType([StructField('tmpop_id_pk', IntegerType(), False),\n",
    "                             StructField('tmpop_fecha', TimestampType(), False),\n",
    "                             StructField('tmpop_anio', ShortType(), False),\n",
    "                             StructField('tmpop_mes_id', ShortType(), False),\n",
    "                             StructField('tmpop_mes', StringType(), False),\n",
    "                             StructField('tmpop_dia', ShortType(), False),\n",
    "                             StructField('tmpop_hora', ShortType(), False)\n",
    "                             ])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Fact_Pot_Disponible():\n",
    "        schema = StructType([StructField('potdisp_id_pk', LongType(), False),\n",
    "                             StructField('agtevt_id_fk', IntegerType(), False),\n",
    "                             StructField('tmpop_id_fk', IntegerType(), False),\n",
    "                             StructField('potdisp_pot_dispon', FloatType(), True)\n",
    "                             ])\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estructuras de Datos Esquemas\n",
    "\n",
    "class EstructurasHDFS:\n",
    "    \"\"\"Clase para obtener de manera estática las estructuras de los dataframes\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_EVENTO():\n",
    "        schema = StructType([StructField('EVENTO_ID', LongType(), True),\n",
    "                             StructField('EVENTO_FECHA', TimestampType(), True),\n",
    "                             StructField('EVENTO_CLASE', ShortType(), True),\n",
    "                             StructField('TPB_EVENTO_ID', ShortType(), True),\n",
    "                             StructField('TPB_CALIF_ID', ShortType(), True),\n",
    "                             StructField('TPB_CLASIF_CALIF_ID', ShortType(), True),\n",
    "                             StructField('TPB_CATEG_CLASIF_ID', ShortType(), True),\n",
    "                             StructField('EVENTO_NUM_REDSP', ShortType(), True),\n",
    "                             StructField('NIVELVOLTAJE_ID', ShortType(), True),\n",
    "                             StructField('EMPRESA_ID', ShortType(), True),\n",
    "                             StructField('ESTACION_ID', ShortType(), True),\n",
    "                             StructField('SAM_ID', IntegerType(), True),\n",
    "                             StructField('SAF_ID', IntegerType(), True),\n",
    "                             StructField('SISTEMA_CLASE', ShortType(), True),\n",
    "                             StructField('USUARIO_ID_CREADOR', ShortType(), True),\n",
    "                             StructField('USUARIO_ID_MODIFICADOR', ShortType(), True),\n",
    "                             StructField('EVENTO_FECHACREACION', TimestampType(), True),\n",
    "                             StructField('EVENTO_FECHAMODIFICACION', TimestampType(), True),\n",
    "                             StructField('NOMBRE_USUARIO_INGRESO', StringType(), True),\n",
    "                             StructField('EVENTO_ES_RELEVANTE', BooleanType(), True),\n",
    "                             StructField('ANIO', ShortType(), True)])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_EVENTO_DTL():\n",
    "        schema = StructType([StructField('EVENTO_ID', LongType(), True),\n",
    "                             StructField('EVENTO_FECHA', TimestampType(), True),\n",
    "                             StructField('EVENTO_DTL_ID', ShortType(), True),\n",
    "                             StructField('EVENTO_DTL_PODER', FloatType(), True),\n",
    "                             StructField('EVENTO_DTL_NOTA', StringType(), True),\n",
    "                             StructField('EVENTO_DTL_ULTIMO', ShortType(), True),\n",
    "                             StructField('ANIO', ShortType(), True)])\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "12680cac-7be4-464a-b092-d15984339c44",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Acceso a Datos\n",
    "\n",
    "from sqlalchemy.sql.expression import func as alchemy_func\n",
    "from sqlalchemy import exc\n",
    "import pandas as pd\n",
    "\n",
    "class TiempoOperacionDA:\n",
    "    \"\"\"Clase para realizar el acceso a datos y persistencia de información de Tiempo Operacion\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dBContextDw = DBContextDw()\n",
    "        self._dBContextDw.Connection()\n",
    "        self.session_maker = self._dBContextDw.session_maker\n",
    "\n",
    "    def GetPkAgtGen(self):\n",
    "        \"\"\"Método para obtener el Id máximo de la tabla\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            max_pk = session.query(alchemy_func.max(Dim_Agt_Gen.agtevt_id_pk)).scalar()\n",
    "            if(max_pk is None):\n",
    "                max_pk = 0\n",
    "            return max_pk\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "            \n",
    "    def GetPkFactPotenciaDisponible(self):\n",
    "        \"\"\"Método para obtener el Id máximo de la tabla de hechos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            max_pk = session.query(alchemy_func.max(Fact_Pot_Disponible.potdisp_id_pk)).scalar()\n",
    "            if(max_pk is None):\n",
    "                max_pk = 0\n",
    "            return max_pk\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetFechaHoraMaximaTiempoOperacion(self):\n",
    "        \"\"\"Método para obtener la fecha máxima de datos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            \n",
    "            max_date = session.query(alchemy_func.max(Fact_Pot_Disponible.tmpop_id_fk)).scalar()\n",
    "            registro = session.query(Dim_Tmp_Operacion).filter(Dim_Tmp_Operacion.tmpop_id_pk == max_date).first()\n",
    "            \n",
    "            if(registro is None):\n",
    "                fecha = '1997-12-31 23:59'\n",
    "            else:\n",
    "                fecha = str(registro.tmpop_fecha)\n",
    "            return fecha\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetAllFechasTiempoOperacion(self, anio=None):\n",
    "        \"\"\"Método para obtener todos todos los datos de la demensión de tiempo\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            if(anio == None):\n",
    "                query = session.query(Dim_Tmp_Operacion) \n",
    "            else:\n",
    "                query = session.query(Dim_Tmp_Operacion).filter(Dim_Tmp_Operacion.tmpop_anio == anio)\n",
    "            df = pd.read_sql(query.statement, query.session.bind)\n",
    "            return df\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetAllAgtGen(self):\n",
    "        \"\"\"Método para obtener todos todos los datos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            query = session.query(Dim_Agt_Gen) \n",
    "            df = pd.read_sql(query.statement, query.session.bind)\n",
    "            return df\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def Save(self, dataframe, nombre_tabla, generic_dataframe):\n",
    "        \"\"\"Método para almacenar las dimensiones y la tabla de hechos en el DW\"\"\"\n",
    "        try:\n",
    "            dataframe.write.jdbc(url=generic_dataframe.url, table=nombre_tabla, mode = generic_dataframe.modo, properties=generic_dataframe.properties)\n",
    "            return True\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "a5e4bc50-cdb9-4e36-bda7-31edcdd89119",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Clases adicionales para refcatorizar codigo\n",
    "\n",
    "class Refactorizar:\n",
    "    \"\"\"Contiene metodos auxiliares del negocio\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def DafaFrameUnidadNegocio(df_empresa,df_cat_empresa,df_cat_unidad_negocio):\n",
    "        try:\n",
    "            df_unidad_negocio = df_empresa.select(col('EMPRESA_ID').alias('UNegocioId'),\n",
    "                                                  col('EMPRESA_CODIGO').alias('UNegocioCodigo'),\n",
    "                                                  col('EMPRESA_NOMBRE').alias('UNegocio'),\n",
    "                                                  col('EMPRESA_CODIGO').alias('EmpresaCodigo'),\n",
    "                                                  col('EMPRESA_NOMBRE').alias('Empresa'))\n",
    "            \n",
    "            df_cat_unidad_negocio = df_cat_unidad_negocio\\\n",
    "            .join(df_cat_empresa, df_cat_unidad_negocio.IdEmpresa == df_cat_empresa.IdEmpresa)\\\n",
    "            .select(df_cat_unidad_negocio.IdUNegocio.alias('UNegocioId'),\n",
    "                    df_cat_unidad_negocio.Codigo.alias('UNegocioCodigo'),\n",
    "                    df_cat_unidad_negocio.Nombre.alias('UNegocio'),\n",
    "                    df_cat_empresa.Codigo.alias('EmpresaCodigo'),\n",
    "                    df_cat_empresa.Nombre.alias('Empresa'))\n",
    "            \n",
    "            return df_cat_unidad_negocio.union(df_unidad_negocio).distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def DafaFrameEstacion(df_central,df_cat_central):\n",
    "        try:\n",
    "            df_central = df_central.select(col('ESTACION_ID').alias('EstacionId'),\n",
    "                                           col('ESTACION_CODIGO').alias('EstacionCodigo'),\n",
    "                                           col('ESTACION_NOMBRE').alias('Estacion'),\n",
    "                                           col('ESTACION_ID').alias('EstacionPadreId'))\n",
    "            \n",
    "            df_cat_central = df_cat_central.select(col('IdCentral').alias('EstacionId'),\n",
    "                                                   col('Codigo').alias('EstacionCodigo'),\n",
    "                                                   col('Nombre').alias('Estacion'),\n",
    "                                                   col('IdCentral').alias('EstacionPadreId'))\n",
    "            \n",
    "            df_cat_central = df_cat_central.union(df_central).distinct()\n",
    "\n",
    "            return df_cat_central.distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def DafaFrameElemento(df_unidad,df_cat_unidad):\n",
    "        try:\n",
    "            df_unidad = df_unidad.select(col('ELEMENTO_ID').alias('ElementoId'),\n",
    "                                         col('ELEMENTO_CODIGO').alias('ElementoCodigo'),\n",
    "                                         col('ELEMENTO_NOMBRE').alias('Elemento'),\n",
    "                                         col('ELEMENTO_TIPO').alias('Tipo'),\n",
    "                                         lit(0).alias('VoltajeId'),\n",
    "                                         col('EMPRESA_ID').alias('UNegocioId'),\n",
    "                                         col('ESTACION_ID').alias('EstacionId'),\n",
    "                                         to_timestamp(when(col('UNIDAD_FECHAOPERACION').isNull(),col('ELEMENTO_FECHACREACION'))\\\n",
    "                                         .otherwise(col('UNIDAD_FECHAOPERACION')),'yyyy-MM-dd HH:mm')\\\n",
    "                                         .alias('FechaInicioOpComercial'),\n",
    "                                         col('ELEMENTO_FECHACREACION').alias('FechaCreacion'),\n",
    "                                         col('UNIDAD_POTENCIAEFECTIVA').alias('PotEfectiva'))\n",
    "            \n",
    "            df_cat_unidad = df_cat_unidad.select(col('IdUnidad').alias('ElementoId'),\n",
    "                                                 col('Codigo').alias('ElementoCodigo'),\n",
    "                                                 col('Nombre').alias('Elemento'),\n",
    "                                                 lit(1).alias('Tipo'),\n",
    "                                                 lit(0).alias('VoltajeId'),\n",
    "                                                 col('IdUNegocio').alias('UNegocioId'),\n",
    "                                                 col('IdCentral').alias('EstacionId'),\n",
    "                                                 to_timestamp(col('FechaInicioOpComercial'),'yyyy-MM-dd HH:mm')\\\n",
    "                                                 .alias('FechaInicioOpComercial'),\n",
    "                                                 col('FechaCreacion'),\n",
    "                                                 col('Pot_Efectiva').alias('PotEfectiva'))\n",
    "            \n",
    "            df_cat_unidad = df_cat_unidad.union(df_unidad).distinct()\n",
    "            \n",
    "            return df_cat_unidad.distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def AgregarDetalles(df_datos,unidad_negocio,centrales,unidades):\n",
    "        try:\n",
    "            df_datos = df_datos\\\n",
    "            .join(unidad_negocio, df_datos.UNegocioId == unidad_negocio.UNegocioId)\\\n",
    "            .join(centrales, df_datos.CentralId == centrales.EstacionId)\\\n",
    "            .join(unidades, df_datos.UnidadId == unidades.ElementoId)\\\n",
    "            .select(unidad_negocio.UNegocioCodigo,\n",
    "                    unidad_negocio.UNegocio,\n",
    "                    unidad_negocio.EmpresaCodigo,\n",
    "                    unidad_negocio.Empresa,\n",
    "                    centrales.EstacionCodigo,\n",
    "                    centrales.Estacion,\n",
    "                    unidades.ElementoCodigo,\n",
    "                    unidades.Elemento,\n",
    "                    unidades.FechaInicioOpComercial,\n",
    "                    unidades.PotEfectiva,\n",
    "                    lit(1).alias('Version'),\n",
    "                    df_datos.Fecha,\n",
    "                    df_datos.PotenciaDisponible).distinct()  \n",
    "            return df_datos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "               \n",
    "    @staticmethod\n",
    "    def AsignarVersion(df_agentes,agentes_dw):\n",
    "        try:\n",
    "            ##### VALIDAR LA VERSIÓN PARA LOS AGENTES DE GENERACIÓN\n",
    "            agt_gen = df_agentes\n",
    "            agt_gen_dw = agentes_dw\n",
    "            \n",
    "            agt_new_version = agt_gen.join(agt_gen_dw,\n",
    "                                           (agt_gen.agtevt_empresa_id_bk == agt_gen_dw.agtevt_empresa_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_unidad_negocio_id_bk == agt_gen_dw.agtevt_unidad_negocio_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_central_id_bk == agt_gen_dw.agtevt_central_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_unidad_id_bk == agt_gen_dw.agtevt_unidad_id_bk))\\\n",
    "            .filter(agt_gen.agtevt_pot_efectiva != agt_gen_dw.agtevt_pot_efectiva)\\\n",
    "            .groupby(agt_gen.agtevt_empresa_id_bk,agt_gen.agtevt_unidad_negocio_id_bk,\n",
    "                     agt_gen.agtevt_central_id_bk,agt_gen.agtevt_unidad_id_bk)\\\n",
    "            .agg(func.max(agt_gen.agtevt_version).alias('agtevt_version'))\n",
    "            \n",
    "            ##### SUMAMOS UNO A LA VERSIÓN DE LOS AGENTES QUE HAN CAMBIADO SU POTENCIA EFECTIVA\n",
    "            df_agentes = df_agentes.join(agt_new_version,\n",
    "                                        (agt_new_version.agtevt_empresa_id_bk == df_agentes.agtevt_empresa_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_unidad_negocio_id_bk == df_agentes.agtevt_unidad_negocio_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_central_id_bk == df_agentes.agtevt_central_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_unidad_id_bk == df_agentes.agtevt_unidad_id_bk), how='left')\\\n",
    "            .select(df_agentes.agtevt_empresa_id_bk,\n",
    "                    df_agentes.agtevt_empresa,\n",
    "                    df_agentes.agtevt_unidad_negocio_id_bk,\n",
    "                    df_agentes.agtevt_unidad_negocio,\n",
    "                    df_agentes.agtevt_central_id_bk,\n",
    "                    df_agentes.agtevt_central,\n",
    "                    df_agentes.agtevt_unidad_id_bk,\n",
    "                    df_agentes.agtevt_unidad,\n",
    "                    df_agentes.agtevt_fecha_oper_comercial.cast(TimestampType()),\n",
    "                    when(agt_new_version.agtevt_version.isNull(),df_agentes.agtevt_version)\\\n",
    "                    .otherwise(agt_new_version.agtevt_version+1).alias('agtevt_version').cast(ShortType()),\n",
    "                    round(df_agentes.agtevt_pot_efectiva.cast(FloatType()),2).alias('agtevt_pot_efectiva'))\n",
    "\n",
    "            return df_agentes\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def AsignarVersionesNuevas(datos_totales,df_agente_gen):\n",
    "        try:\n",
    "            datos_totales = datos_totales\\\n",
    "            .join(df_agente_gen,\n",
    "                  (datos_totales.EmpresaCodigo == df_agente_gen.agtevt_empresa_id_bk) &\\\n",
    "                  (datos_totales.UNegocioCodigo == df_agente_gen.agtevt_unidad_negocio_id_bk) &\\\n",
    "                  (datos_totales.EstacionCodigo == df_agente_gen.agtevt_central_id_bk) &\\\n",
    "                  (datos_totales.ElementoCodigo == df_agente_gen.agtevt_unidad_id_bk) &\\\n",
    "                  (datos_totales.PotEfectiva == df_agente_gen.agtevt_pot_efectiva), how='left')\\\n",
    "            .select(datos_totales.EmpresaCodigo,\n",
    "                    datos_totales.Empresa,\n",
    "                    datos_totales.UNegocioCodigo,\n",
    "                    datos_totales.UNegocio,\n",
    "                    datos_totales.EstacionCodigo,\n",
    "                    datos_totales.Estacion,\n",
    "                    datos_totales.ElementoCodigo,\n",
    "                    datos_totales.Elemento,\n",
    "                    datos_totales.PotEfectiva,\n",
    "                    when(df_agente_gen.agtevt_pot_efectiva.isNull(),datos_totales.Version)\\\n",
    "                    .otherwise(df_agente_gen.agtevt_version).alias('Version'),\n",
    "                    datos_totales.Fecha,\n",
    "                    datos_totales.PotenciaDisponible)\n",
    "            \n",
    "            return datos_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def CalcularHorasIntermedias(x):\n",
    "        try:\n",
    "            id=x[0]\n",
    "            datos=list(x[1])\n",
    "            anioActual = int(id.split('-')[3])\n",
    "            datosProcesamiento = []\n",
    "            datosSalida = []\n",
    "            datoBorrado = None  \n",
    "\n",
    "            fechaRegistroInicial = datos[0][0]\n",
    "            estadoRegistroInicial = datos[0][1]\n",
    "            fechaCreacionInicial = datos[0][2]\n",
    "\n",
    "            #EL ESTADO Y LA POTENCIA ANTERIORES AL QUE QUEDA ESTARÁN EN EL REGISTRO BORRADO\n",
    "\n",
    "            #Primero si los minutos del primer registro es diferente de cero entonces:\n",
    "            #Agrego el limiter inferior con hora y minuto cero y con estado contrario al primer registro\n",
    "            if(fechaRegistroInicial.minute!=0):\n",
    "                fechaInicial = fechaRegistroInicial - timedelta(minutes=fechaRegistroInicial.minute)\n",
    "                estadoAnterior = datos[0][4]\n",
    "                potenciaAnterior = datos[0][5]\n",
    "                datosProcesamiento.append([fechaInicial,estadoAnterior,fechaCreacionInicial,potenciaAnterior,False,'0.0'])\n",
    "\n",
    "\n",
    "            #Agrego la data ingresada\n",
    "            datosProcesamiento += datos\n",
    "\n",
    "            #Siempre agrego el limite superior con la hora y minuto en cero (la siguiente hora),\n",
    "            #con estado cambiado del ultimo registro\n",
    "            fechaRegistroFinal = datos[-1][0]\n",
    "            estadoRegistroFinal = datos[-1][1]\n",
    "            fechaCreacionFinal = datos[-1][2]\n",
    "            potenciaRegistroFinal = datos[-1][3]\n",
    "\n",
    "            fechaFinal = (fechaRegistroFinal - timedelta(minutes=fechaRegistroFinal.minute)) + timedelta(hours=1)\n",
    "\n",
    "            datosProcesamiento.append([fechaFinal,estadoRegistroFinal,fechaCreacionFinal,potenciaRegistroFinal,estadoRegistroFinal,potenciaRegistroFinal])\n",
    "\n",
    "            #print(datosProcesamiento)\n",
    "            #Iteramos, tomo el elemento i-1 y el elemento i, i el estado i es falso, el tiempo es la diferencia,\n",
    "            #caso contrario el tiempo es cero y saco el registro i\n",
    "            for i in range(0,len(datosProcesamiento)):\n",
    "                if(i==0):\n",
    "                    fecha = datosProcesamiento[i][0]\n",
    "                    estado = datosProcesamiento[i][1]\n",
    "                    fechaCreacion = datosProcesamiento[i][2]\n",
    "                    potencia = datosProcesamiento[i][3]\n",
    "                    tiempo = 0.0\n",
    "                    datosSalida.append([fecha,tiempo,estado,fechaCreacion,potencia,False,0.0,0.0])\n",
    "                    continue\n",
    "\n",
    "                datoAnterior = datosProcesamiento[i-1]\n",
    "                datoActual = datosProcesamiento[i]\n",
    "\n",
    "                fechaAnterior = datoAnterior[0]\n",
    "\n",
    "                fechaActual = datoActual[0]\n",
    "                estadoActual = datoActual[1]\n",
    "                potenciaActual = datoActual[3]\n",
    "                fechaCreacionActual = datoActual[2]\n",
    "\n",
    "                estadoAnterior = datoActual[4]\n",
    "                potenciaAnterior = float(datoActual[5])\n",
    "\n",
    "                if(estadoAnterior is True):\n",
    "                    tiempo = divmod((fechaActual-fechaAnterior).total_seconds(), 60)[0]\n",
    "                    potenciaDisponible = (tiempo/60.0)*potenciaAnterior\n",
    "                    datosSalida.append([fechaActual,tiempo,estadoActual,fechaCreacionActual,potenciaActual,estadoAnterior,potenciaAnterior,potenciaDisponible])\n",
    "                else:\n",
    "                    tiempo = 0.0\n",
    "                    potenciaDisponible = 0.0\n",
    "                    datosSalida.append([fechaActual,tiempo,estadoActual,fechaCreacionActual,potenciaActual,estadoAnterior,potenciaAnterior,potenciaDisponible])\n",
    "\n",
    "            #Sumo el tiempo del último registro al tiempo del penultimo registro\n",
    "            datosSalida[-2][1] += datosSalida[-1][1]\n",
    "            datosSalida[-2][7] += datosSalida[-1][7]\n",
    "\n",
    "            #si los datos de salida tienen dos elementos, devuelvo el primer registro, caso contrario devuelvo\n",
    "            #los datos desde el segundo registro al penultimo registro\n",
    "            if(len(datosSalida)==2):\n",
    "                datosSalida = datosSalida[0:1]\n",
    "            else:\n",
    "                datosSalida = datosSalida[1:-1]  \n",
    "            return (id,datosSalida)\n",
    "        except Exception as error:\n",
    "                ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def RellenarFechas(x):\n",
    "        try:\n",
    "            id = x[0]\n",
    "            datos = list(x[1])\n",
    "\n",
    "            anio = datos[0][0].year\n",
    "            fechaCreacion = datos[0][3]\n",
    "            datosSalida = []\n",
    "            fechaInicioAnio = datetime.datetime(anio,1,1,0,0,0)\n",
    "            fechaInicioAnioSiguiente = datetime.datetime(anio+1,1,1,0,0,0)\n",
    "\n",
    "            #Validamos: 1.- Si el es el primero recorremos e insertamos las horas desde el registro 1 hasta el inicio del año\n",
    "            for i in range(0,len(datos)):\n",
    "                tiempo = 0.0\n",
    "                if(i == 0):\n",
    "                    fechaLimiteInferior = datos[i][0] - (timedelta(minutes=datos[i][0].minute) + timedelta(hours=1))\n",
    "                    estadoAnterior = datos[i][5]\n",
    "                    potenciaAnterior = float(datos[i][6])\n",
    "                    \n",
    "                    if(estadoAnterior is True):\n",
    "                        tiempo = 60.0\n",
    "                        \n",
    "                    if(fechaCreacion>fechaInicioAnio):\n",
    "                        fechaInicioAnio = fechaCreacion\n",
    "                        \n",
    "                    if(fechaLimiteInferior>fechaInicioAnio):\n",
    "                        while(fechaLimiteInferior>=fechaInicioAnio):\n",
    "                            potenciaDisponible = (tiempo/60.0)*potenciaAnterior\n",
    "                            datosSalida.append([fechaLimiteInferior,tiempo,estadoAnterior,fechaCreacion,potenciaAnterior,False,0.0,potenciaDisponible])\n",
    "                            fechaLimiteInferior -= timedelta(hours=1)     \n",
    "                else:\n",
    "                    fechaAnterior = datos[i-1][0]\n",
    "                    estadoAnterior = datos[i-1][2]\n",
    "                    potenciaAnterior = float(datos[i-1][4])\n",
    "                    \n",
    "                    fechaActual = datos[i][0]\n",
    "                    \n",
    "                    fechaAnteriorSinMinutos = fechaAnterior - timedelta(minutes=fechaAnterior.minute) \n",
    "                    fechaActualSinMinutos = fechaActual - timedelta(minutes=fechaActual.minute) \n",
    "                    \n",
    "                    diferencia_tiempo = divmod((fechaActual-fechaAnterior).total_seconds(), 60)[0]\n",
    "                    \n",
    "                    if(estadoAnterior is True):\n",
    "                        tiempo = 60.0\n",
    "                        \n",
    "                    if(not ((diferencia_tiempo<60.0) & (fechaAnteriorSinMinutos == fechaActualSinMinutos))):\n",
    "                        fechaActualSinMinutos -= timedelta(hours=1)\n",
    "                        while(fechaActualSinMinutos>fechaAnterior):\n",
    "                            potenciaDisponible = (tiempo/60.0)*potenciaAnterior\n",
    "                            datosSalida.append([fechaActualSinMinutos,tiempo,estadoAnterior,fechaCreacion,potenciaAnterior,estadoAnterior,potenciaAnterior,potenciaDisponible])\n",
    "                            fechaActualSinMinutos -= timedelta(hours=1)\n",
    "                    #SI ES EL ÚLTIMO REGISTRO, RECORRO PARA ABAJO HASTA EL COMIENZO DEL SIGUIENTE AÑO\n",
    "                    if(i==len(datos)-1):\n",
    "                        fechaActualSistema = datetime.datetime.now()\n",
    "                        fechaLimiteSuperior = datos[i][0] + (timedelta(minutes=60)-timedelta(minutes=datos[i][0].minute))\n",
    "                        estadoActual = datos[i][2]\n",
    "                        potenciaActual = float(datos[i][4])\n",
    "                        \n",
    "                        if(estadoActual):\n",
    "                            tiempo = 60.0\n",
    "                            \n",
    "                        #RECORRO PARA ABAJO\n",
    "                        if(fechaInicioAnioSiguiente>fechaActualSistema):\n",
    "                            fechaInicioAnioSiguiente = fechaActualSistema\n",
    "                            \n",
    "                        if(fechaLimiteSuperior<fechaInicioAnioSiguiente):\n",
    "                            while(fechaLimiteSuperior<fechaInicioAnioSiguiente):\n",
    "                                potenciaDisponible = (tiempo/60.0)*potenciaActual\n",
    "                                datosSalida.append([fechaLimiteSuperior,tiempo,estadoActual,fechaCreacion,potenciaActual,estadoActual,potenciaActual,potenciaDisponible])\n",
    "                                fechaLimiteSuperior += timedelta(hours=1)\n",
    "\n",
    "            datos += datosSalida\n",
    "\n",
    "            return(id,datos)\n",
    "        except Exception as error:\n",
    "                ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def SepararDatos(x):\n",
    "        try:\n",
    "            id = x[0].split('-') \n",
    "            datos = list(x[1])\n",
    "            splitDatos = [(id[0],id[1],id[2],d[0],d[1],d[2],d[3],d[4],d[5],d[6]) for d in datos]\n",
    "            return splitDatos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def SepararDatosCalculados(x):\n",
    "        try:\n",
    "            id = x[0].split('-') \n",
    "            datos = list(x[1])\n",
    "            splitDatos = [(id[0],id[1],id[2],d[0],d[1],d[2],d[3],d[4],d[5],d[6],d[7]) for d in datos]\n",
    "            return splitDatos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def SumarizarTiempos(df_datos_tiempos):\n",
    "        try:\n",
    "            #df_datos_tiempos.filter((func.year(col('Fecha'))==2018) &\\\n",
    "            #                        (func.month(col('Fecha'))== 7)&\\\n",
    "            #                        (func.dayofmonth(col('Fecha'))==19) &\\\n",
    "            #                        (func.hour(col('Fecha'))==11)).show(1000)\n",
    "            df_datos_tiempos = df_datos_tiempos\\\n",
    "            .groupby('UNegocioId',\n",
    "                     'CentralId',\n",
    "                     'UnidadId',\n",
    "                     func.year(col('Fecha')).alias('Anio'),\n",
    "                     func.month(col('Fecha')).alias('Mes'),\n",
    "                     func.dayofmonth(col('Fecha')).alias('Dia'),\n",
    "                     func.hour(col('Fecha')).alias('Hora'),\n",
    "                     func.from_unixtime(unix_timestamp('Fecha'), 'yyyy-MM-dd HH:00:00').alias('Fecha'))\\\n",
    "            .agg(func.sum('PotenciaDisponible').alias('PotenciaDisponible'))\n",
    "            #df_datos_tiempos.filter(col('PotenciaDisponible')>60.0).show(1000)\n",
    "            return df_datos_tiempos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def FiltrarEventos(eventos,anio,unidades):\n",
    "        try:\n",
    "            eventosActuales = eventos\\\n",
    "            .filter(eventos.Anio==anio)\\\n",
    "            .select(eventos.EventoFecha.alias('PseudoEventoFechaActual'),\n",
    "                    eventos.EventoFecha.alias('EventoFechaActual'),\n",
    "                    eventos.Anio.alias('AnioActual'),\n",
    "                    eventos.EvtId.alias('EvtIdActual'),\n",
    "                    eventos.EvtCodigo.alias('EvtCodigoActual'),\n",
    "                    eventos.CausalId.alias('CausalIdActual'),\n",
    "                    eventos.CausalCodigo.alias('CausalCodigoActual'),\n",
    "                    eventos.UNegocioId.alias('UNegocioIdActual'),\n",
    "                    eventos.CentralId.alias('CentralIdActual'),\n",
    "                    eventos.UnidadId.alias('UnidadIdActual'),\n",
    "                    eventos.FechaCreacionEvento.alias('FechaCreacionEventoActual'),\n",
    "                    eventos.FechaCreacionElemento.alias('FechaCreacionElementoActual'),\n",
    "                    eventos.Potencia.alias('PotenciaActual'),\n",
    "                    eventos.ONOFF.alias('ONOFFActual'),\n",
    "                    eventos.OFFON.alias('OFFONActual'))\n",
    "            \n",
    "            unidadFecha = eventosActuales\\\n",
    "            .groupby(col('UnidadIdActual').alias('Unidad'))\\\n",
    "            .agg(func.min('EventoFechaActual').alias('Fecha'))\n",
    "            \n",
    "            eventosAnteriores = eventos\\\n",
    "            .join(unidadFecha,eventos.UnidadId==unidadFecha.Unidad)\\\n",
    "            .filter((eventos.EventoFecha<unidadFecha.Fecha) &\\\n",
    "                    ((eventos.ONOFF==True)|(eventos.OFFON==True)))\\\n",
    "            .groupby(eventos.UnidadId.alias('IdUnidad'))\\\n",
    "            .agg(func.max(eventos.EventoFecha).alias('FechaEvento'),\n",
    "                 func.min(unidadFecha.Fecha).alias('FechaLimiteSuperior'),\n",
    "                 func.max(eventos.FechaCreacionEvento).alias('CreacionFechaEvento'))\n",
    "            \n",
    "            eventosAnioAnterior = eventos\\\n",
    "            .join(eventosAnteriores,\n",
    "                  (eventos.UnidadId==eventosAnteriores.IdUnidad) &\\\n",
    "                  (eventos.EventoFecha==eventosAnteriores.FechaEvento) &\\\n",
    "                  (eventos.FechaCreacionEvento==eventosAnteriores.CreacionFechaEvento))\\\n",
    "            .filter((eventos.ONOFF==True)|(eventos.OFFON==True))\\\n",
    "            .select(func.from_unixtime(unix_timestamp('FechaLimiteSuperior'), 'yyyy-MM-dd HH:00:00').alias('PseudoEventoFechaActual'),\n",
    "                    eventos.EventoFecha.alias('EventoFechaActual'),\n",
    "                    eventos.Anio.alias('AnioActual'),\n",
    "                    eventos.EvtId.alias('EvtIdActual'),\n",
    "                    eventos.EvtCodigo.alias('EvtCodigoActual'),\n",
    "                    eventos.CausalId.alias('CausalIdActual'),\n",
    "                    eventos.CausalCodigo.alias('CausalCodigoActual'),\n",
    "                    eventos.UNegocioId.alias('UNegocioIdActual'),\n",
    "                    eventos.CentralId.alias('CentralIdActual'),\n",
    "                    eventos.UnidadId.alias('UnidadIdActual'),\n",
    "                    eventos.FechaCreacionEvento.alias('FechaCreacionEventoActual'),\n",
    "                    eventos.FechaCreacionElemento.alias('FechaCreacionElementoActual'),\n",
    "                    eventos.Potencia.alias('PotenciaActual'),\n",
    "                    eventos.ONOFF.alias('ONOFFActual'),\n",
    "                    eventos.OFFON.alias('OFFONActual'))\n",
    "\n",
    "            eventos = eventosActuales.union(eventosAnioAnterior)\n",
    "            eventosTotales = eventos\\\n",
    "            .select(eventos.PseudoEventoFechaActual.alias('PseudoEventoFecha').cast(TimestampType()),\n",
    "                    eventos.EventoFechaActual.alias('EventoFecha'),\n",
    "                    eventos.AnioActual.alias('Anio'),\n",
    "                    eventos.EvtIdActual.alias('EvtId'),\n",
    "                    eventos.EvtCodigoActual.alias('EvtCodigo'),\n",
    "                    eventos.CausalIdActual.alias('CausalId'),\n",
    "                    eventos.CausalCodigoActual.alias('CausalCodigo'),\n",
    "                    eventos.UNegocioIdActual.alias('UNegocioId'),\n",
    "                    eventos.CentralIdActual.alias('CentralId'),\n",
    "                    eventos.UnidadIdActual.alias('UnidadId'),\n",
    "                    eventos.FechaCreacionEventoActual.alias('FechaCreacionEvento'),\n",
    "                    eventos.FechaCreacionElementoActual.alias('FechaCreacionElemento'),\n",
    "                    eventos.PotenciaActual.alias('Potencia'),\n",
    "                    eventos.ONOFFActual.alias('ONOFF'),\n",
    "                    eventos.OFFONActual.alias('OFFON'))\\\n",
    "            .orderBy('UnidadId','EventoFecha','FechaCreacionEvento')\n",
    "\n",
    "            return eventosTotales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def AgregarRegistroAnterior(x):\n",
    "        try:\n",
    "            id = x[0]\n",
    "            datos = list(x[1])\n",
    "            datosSalida=[]\n",
    "\n",
    "            for i in range(0,len(datos)):\n",
    "                if(i == 0):\n",
    "                    datosSalida.append([datos[i][0],datos[i][1],datos[i][2],datos[i][3],datos[i][4],False,'0.0'])\n",
    "                    continue\n",
    "                registroAnterior = datos[i-1]\n",
    "                datosSalida.append([datos[i][0],datos[i][1],datos[i][2],datos[i][3],datos[i][4],registroAnterior[2],registroAnterior[4]])\n",
    "\n",
    "            return (id,datosSalida)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREACIÓN DE VISTAS\n",
    "\n",
    "class CreacionVistas:\n",
    "    \"\"\"Otorga método rapidos para formar las vistas de datos que se necesitan para realizar cálculos\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def CrearvRepBOSNIExtendONOFF(df_evento,df_evento_dtl,df_calif,df_evt,df_mtx,df_clasif,df_categ,unidades):\n",
    "        try:\n",
    "            df_evento = df_evento.filter(df_evento.EVENTO_CLASE==4)\\\n",
    "            .withColumn('CALIF_ID',\n",
    "                        when(df_evento.TPB_CALIF_ID.isNull(),59)\\\n",
    "                        .otherwise(when(df_evento.TPB_CALIF_ID==0,59).otherwise(df_evento.TPB_CALIF_ID)))\n",
    "            \n",
    "            eventos = df_evento\\\n",
    "            .join(df_evento_dtl, df_evento.EVENTO_ID == df_evento_dtl.EVENTO_ID)\\\n",
    "            .join(df_calif, df_evento.CALIF_ID == df_calif.TPB_CALIF_ID, how='left')\\\n",
    "            .join(df_evt, df_evento.TPB_EVENTO_ID == df_evt.TPB_EVENTO_ID, how='left')\\\n",
    "            .join(df_mtx,\n",
    "                  (df_evento.TPB_EVENTO_ID == df_mtx.TPB_EVENTO_ID) &\\\n",
    "                  (df_evento.CALIF_ID == df_mtx.TPB_CALIF_ID), how='left')\\\n",
    "            .select(to_timestamp(df_evento.EVENTO_FECHA,'yyyy-MM-dd HH:mm').alias('EventoFecha'),\n",
    "                    df_evento.ANIO.alias('Anio'),\n",
    "                    df_evento.TPB_EVENTO_ID.alias('EvtId'),\n",
    "                    df_evt.TPB_EVENTO_CODIGO.alias('EvtCodigo'),\n",
    "                    df_evento.TPB_CALIF_ID.alias('CausalId'),\n",
    "                    df_calif.TPB_CALIF_CODIGO.alias('CausalCodigo'), \n",
    "                    df_evento.EMPRESA_ID.alias('UNegocioId'),\n",
    "                    df_evento.ESTACION_ID.alias('CentralId'),\n",
    "                    df_evento_dtl.EVENTO_DTL_ID.alias('UnidadId'),\n",
    "                    when(df_evento_dtl.EVENTO_DTL_PODER.isNull(),0)\\\n",
    "                    .otherwise(df_evento_dtl.EVENTO_DTL_PODER).alias('Potencia'),\n",
    "                    when(df_evt.TPB_EVENTO_CALCULO_ON.isNull(),False)\\\n",
    "                    .otherwise(df_evt.TPB_EVENTO_CALCULO_ON).alias('CalculoON'),\n",
    "                    when(df_evt.TPB_EVENTO_CALCULO_OFF.isNull(),False)\\\n",
    "                    .otherwise(df_evt.TPB_EVENTO_CALCULO_OFF).alias('CalculoOFF'),\n",
    "                    when(df_mtx.TPB_MTX_ON_OFF.isNull(),False)\\\n",
    "                    .otherwise(df_mtx.TPB_MTX_ON_OFF).alias('ONOFF'),\n",
    "                    when(df_mtx.TPB_MTX_OFF_ON.isNull(),False)\\\n",
    "                    .otherwise(df_mtx.TPB_MTX_OFF_ON).alias('OFFON'),\n",
    "                    df_evento.EVENTO_FECHACREACION.alias('FechaEventoCreacion')) \n",
    "            \n",
    "            ###### FILTRAMOS SOLO EVENTOS DONDE INTERVIENEN CALCULOS ON y OFF\n",
    "            eventos = eventos.filter((eventos.ONOFF==True) | (eventos.OFFON==True))\\\n",
    "            .orderBy('UnidadId','EventoFecha')\n",
    "\n",
    "            eventos = eventos\\\n",
    "            .join(unidades, eventos.UnidadId == unidades.ElementoId)\\\n",
    "            .select(eventos.EventoFecha,\n",
    "                    eventos.Anio,\n",
    "                    eventos.EvtId,\n",
    "                    eventos.EvtCodigo,\n",
    "                    eventos.CausalId,\n",
    "                    eventos.CausalCodigo, \n",
    "                    eventos.UNegocioId,\n",
    "                    eventos.CentralId,\n",
    "                    eventos.UnidadId,\n",
    "                    to_timestamp(unidades.FechaCreacion,'yyyy-MM-dd HH:mm').alias('FechaCreacionElemento'),\n",
    "                    to_timestamp(eventos.FechaEventoCreacion).alias('FechaCreacionEvento'),\n",
    "                    eventos.Potencia,\n",
    "                    eventos.CalculoON,\n",
    "                    eventos.CalculoOFF,\n",
    "                    eventos.ONOFF,\n",
    "                    eventos.OFFON)\n",
    "            \n",
    "            return eventos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "azdata_cell_guid": "db61d617-5b54-4826-b85c-4fb0c4e13676"
   },
   "outputs": [],
   "source": [
    "## Lógica de Negocio de Interrupciones del SNI\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, asc, concat, split, udf, regexp_replace,to_date,to_timestamp,round,\\\n",
    "                                  UserDefinedFunction, array, explode, struct, lit, trim, when, unix_timestamp\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import Window\n",
    "import re\n",
    "\n",
    "\n",
    "class TiempoOperacionBI:\n",
    "    \"\"\"Clase para manejar la lógica del ETL para Energía No Suministrada del SNI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self._fallasSniDA = TiempoOperacionDA()\n",
    "            self._genericDataFrame = None\n",
    "            self._df_tiempo_operacion = None\n",
    "            self._df_agt_gen = None\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def PoseeArchivos(self,rdd):\n",
    "        try:\n",
    "            num = rdd.count()\n",
    "            return num>0\n",
    "        except Exception as error:\n",
    "            return False\n",
    "    \n",
    "    def GetDataFrameHdfs(self,table_name,file_name):\n",
    "        try:\n",
    "            df = self._genericDataFrame.GetDataHdfs(table_name,file_name)\n",
    "            return df\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return None \n",
    "        \n",
    "    \n",
    "    def GetData(self,directoryNme,anio=None,schema=None):\n",
    "        \"\"\"Método para recolectar datos de un directorio específico\"\"\"\n",
    "        try:\n",
    "            sc = self._genericDataFrame.spark.sparkContext\n",
    "            \n",
    "            if anio==None:\n",
    "                ruta = self._genericDataFrame.hdfsContext.HdfsPath(directoryNme,\"file_\"+directoryNme+'*')\n",
    "                archivo = \"file_\"+directoryNme+'*'\n",
    "            else:\n",
    "                ruta = self._genericDataFrame.hdfsContext.HdfsPath(directoryNme,\"file_\"+directoryNme+'_'+str(anio))\n",
    "                archivo = \"file_\"+directoryNme+'_'+str(anio)\n",
    "                \n",
    "            rdd = sc.wholeTextFiles(ruta)\n",
    "            posee = self.PoseeArchivos(rdd)\n",
    "            \n",
    "            if posee==False:\n",
    "                return self._genericDataFrame.spark.createDataFrame([],schema)\n",
    "            \n",
    "            return self._genericDataFrame.GetDataHdfs(directoryNme,archivo)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return None \n",
    "    \n",
    "    def ProcesarDatos(self,anio=None):\n",
    "        \"\"\"Método principal en el que se realiza la limpieza y tratamiento de lo datos\"\"\"\n",
    "        \n",
    "        try:\n",
    "            str_fecha = self._fallasSniDA.GetFechaHoraMaximaTiempoOperacion() \n",
    "            if(anio == None):\n",
    "                anio = datetime.datetime.strptime(str_fecha,'%Y-%m-%d %H:%M').year + 1\n",
    "\n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='BDTREV2'))\n",
    "            table_empresa = 'EMPRESA'\n",
    "            table_central = 'CENTRAL'\n",
    "            table_unidad = 'UNIDAD'\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_empresa)\n",
    "            df_empresa = self.GetDataFrameHdfs(table_empresa,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_central)\n",
    "            df_central = self.GetDataFrameHdfs(table_central,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_unidad)\n",
    "            df_unidad = self.GetDataFrameHdfs(table_unidad,file_name)\n",
    "            \n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='SIVO'))\n",
    "            table_cat_empresa = 'CFG_Empresa'\n",
    "            table_cat_unidad_negocio = 'CFG_UnidadNegocio'\n",
    "            table_cat_central = 'CFG_Central'\n",
    "            table_cat_unidad = 'CFG_Unidad'\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_empresa)\n",
    "            df_cat_empresa = self.GetDataFrameHdfs(table_cat_empresa,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_unidad_negocio)\n",
    "            df_cat_unidad_negocio = self.GetDataFrameHdfs(table_cat_unidad_negocio,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_central)\n",
    "            df_cat_central = self.GetDataFrameHdfs(table_cat_central,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_unidad)\n",
    "            df_cat_unidad = self.GetDataFrameHdfs(table_cat_unidad,file_name)\n",
    "            \n",
    "            \n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='BOSNI'))\n",
    "            table_causal = 'TPB_CALIF'\n",
    "            table_evento = 'TPB_EVENTO'\n",
    "            table_clasificacion = 'TPB_CLASIFICACION_CALIF'\n",
    "            table_categoria = 'TPB_CATEGORIA_CLASIF'\n",
    "            table_mtx_calif_evento = 'MTX_CALIF_EVENTO'\n",
    "            \n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_causal)\n",
    "            df_calif = self.GetDataFrameHdfs(table_causal,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_evento)\n",
    "            df_evt = self.GetDataFrameHdfs(table_evento,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_clasificacion)\n",
    "            df_clasif = self.GetDataFrameHdfs(table_clasificacion,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_categoria)\n",
    "            df_categ = self.GetDataFrameHdfs(table_categoria,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_mtx_calif_evento)\n",
    "            df_mtx = self.GetDataFrameHdfs(table_mtx_calif_evento,file_name)\n",
    "\n",
    "            print('Inicio: ' + str(datetime.datetime.now()))\n",
    "\n",
    "            directoryNme = 'EVENTO'\n",
    "            df_evento = self.GetData(directoryNme,None,EstructurasHDFS.Schema_EVENTO())\n",
    "            \n",
    "            directoryNme = 'EVENTO_DTL'\n",
    "            df_evento_dtl = self.GetData(directoryNme,None,EstructurasHDFS.Schema_EVENTO_DTL())\n",
    "            \n",
    "            ################################### CATÁLOGO DE UNIDADES DE NEGOCIO\n",
    "            unidad_negocio = Refactorizar.DafaFrameUnidadNegocio(df_empresa,df_cat_empresa,df_cat_unidad_negocio)\n",
    "            \n",
    "            ################################### CENTRAL         \n",
    "            centrales = Refactorizar.DafaFrameEstacion(df_central,df_cat_central)\n",
    "            \n",
    "            ################################### UNIDAD\n",
    "            unidades = Refactorizar.DafaFrameElemento(df_unidad,df_cat_unidad)\n",
    "                \n",
    "            ################################### DATOS DE VISTAS\n",
    "            vBosniExtend = CreacionVistas.CrearvRepBOSNIExtendONOFF(df_evento,df_evento_dtl,df_calif,df_evt,\n",
    "                                                                    df_mtx,df_clasif,df_categ,unidades)\n",
    "            \n",
    "            ################################### DATOS DE VISTAS FILTRADO POR ANIO Y AGREGAMOS LOS EVENTOS ANTERIORES\n",
    "            vBosniExtend = Refactorizar.FiltrarEventos(vBosniExtend,anio,unidades)\n",
    "            \n",
    "            #vBosniExtend.filter(vBosniExtend.UnidadId==2729).show(vBosniExtend.count())\n",
    "            \n",
    "            ################################### CÁLCULO DEL TIEMPO DE OPERACIÓN DE LAS UNIDADES\n",
    "            print('Proceso de generación de datos: ' + str(datetime.datetime.now()))\n",
    "            \n",
    "            #vBosniExtend = vBosniExtend.filter(vBosniExtend.UnidadId.isin({2729}))\n",
    "            \n",
    "            ################################## AGREGAMOS EN CADA REGISTRO EL ESTADO Y LA POTENCIA DEL EVENTO ANTERIOR\n",
    "            datosMap = vBosniExtend.rdd\\\n",
    "            .map(lambda x: (str(x.UNegocioId)+'-'+str(x.CentralId)+'-'+str(x.UnidadId),\n",
    "                            [x.PseudoEventoFecha,x.EventoFecha,x.ONOFF,x.FechaCreacionElemento,x.Potencia]))\n",
    "            \n",
    "            datosAgrupadosPorElemento = datosMap.groupByKey()\n",
    "            \n",
    "            datosMapAgregados = datosAgrupadosPorElemento.map(lambda x: Refactorizar.AgregarRegistroAnterior(x))\n",
    "            \n",
    "            rddDatosSeparados = datosMapAgregados.flatMap(lambda x: Refactorizar.SepararDatos(x))\n",
    "            \n",
    "            df_vBosniExtend = rddDatosSeparados\\\n",
    "            .toDF(['UNegocioId','CentralId','UnidadId',\n",
    "                   'PseudoEventoFecha','EventoFecha','EstadoActual','FechaCreacionElemento','PotenciaActual',\n",
    "                   'EstadoAnterior','PotenciaAnterior'])\\\n",
    "            .orderBy('UnidadId','EventoFecha')\n",
    "            \n",
    "            df_vBosniExtend = df_vBosniExtend.filter(func.year(col('EventoFecha'))==anio)\n",
    "            \n",
    "            ################################## CALCULAMOS LOS TIEMPOS DE OPERACIÓN DE LAS HORAS DE EVENTOS\n",
    "            datosValueKey = df_vBosniExtend.rdd\\\n",
    "            .map(lambda x: (str(x.UNegocioId)+'-'+str(x.CentralId)+'-'+str(x.UnidadId)+'-'+\n",
    "                            str(x.PseudoEventoFecha.year)+'-'+str(x.PseudoEventoFecha.month)+'-'+\n",
    "                            str(x.PseudoEventoFecha.day)+'-'+str(x.PseudoEventoFecha.hour),\n",
    "                            [x.EventoFecha,x.EstadoActual,x.FechaCreacionElemento,x.PotenciaActual,\n",
    "                             x.EstadoAnterior,x.PotenciaAnterior]))\n",
    "\n",
    "            datosGroupValueByKey = datosValueKey.groupByKey()\n",
    "\n",
    "            datosConTiemposCalculados = datosGroupValueByKey.map(lambda x: Refactorizar.CalcularHorasIntermedias(x))\n",
    "\n",
    "            rddSeparadoConTiemposCalculados = datosConTiemposCalculados.flatMap(lambda x: Refactorizar.SepararDatosCalculados(x))\n",
    "            \n",
    "            df_datos_separados = rddSeparadoConTiemposCalculados\\\n",
    "            .toDF(['UNegocioId','CentralId','UnidadId','EventoFecha','Tiempo','EstadoActual',\n",
    "                   'FechaCreacionElemento','PotenciaActual',\n",
    "                   'EstadoAnterior','PotenciaAnterior','PotenciaDisponible'])\\\n",
    "            .orderBy('UnidadId','EventoFecha')\n",
    "\n",
    "            ################################## RELLENAMOS LAS HORAS QUE FALTAN CON LA POTENCIA DE OPERACIÓN\n",
    "            rddValueKeyUnidad = df_datos_separados.rdd\\\n",
    "            .map(lambda x: (x.UNegocioId+'-'+x.CentralId+'-'+x.UnidadId,\n",
    "                            [x.EventoFecha,x.Tiempo,x.EstadoActual,x.FechaCreacionElemento,\n",
    "                             x.PotenciaActual,x.EstadoAnterior,x.PotenciaAnterior,x.PotenciaDisponible]))\n",
    "            \n",
    "            rddGroupByUnidad = rddValueKeyUnidad.groupByKey()\n",
    "            \n",
    "            rddDatosTotales = rddGroupByUnidad.map(lambda x: Refactorizar.RellenarFechas(x))\n",
    "            \n",
    "            rddSeparadoTotales = rddDatosTotales.flatMap(lambda x: Refactorizar.SepararDatosCalculados(x))\n",
    "            \n",
    "            df_datos_tiempos = rddSeparadoTotales\\\n",
    "            .toDF(['UNegocioId','CentralId','UnidadId','Fecha','Tiempo','EstadoActual',\n",
    "                   'FechaCreacionElemento','PotenciaActual',\n",
    "                   'EstadoAnterior','PotenciaAnterior','PotenciaDisponible'])\\\n",
    "            .orderBy('UnidadId','Fecha')\n",
    "            \n",
    "            #df_datos_tiempos.show(1000)\n",
    "            ############################## SUMAR HORARIAMENTE POR CADA UNIDAD EL TIEMPO DE OPERACIÓN\n",
    "            df_datos_tiempos = Refactorizar.SumarizarTiempos(df_datos_tiempos)\n",
    "            \n",
    "            ############################## PROCESAR AGENTES\n",
    "            datos_totales = self.AgregarAgentes(df_datos_tiempos,unidad_negocio,centrales,unidades)\n",
    "            \n",
    "            ################################### LIMPIEZA DE DIMENSIONES\n",
    "            ################################### LIMPIEZA DE AGENTES\n",
    "            print('Limpiar Agentes')\n",
    "            agentes = self.LimpiarAgentes(datos_totales)\n",
    "            \n",
    "            ##############################  ASIGNACIONES DE VERSIONES\n",
    "            print('Asignar Versiones')\n",
    "            datos_totales = Refactorizar.AsignarVersionesNuevas(datos_totales,self._df_agt_gen)\n",
    "            \n",
    "            ##############################  LIMPIEZ DE TIEMPO DE OPERACION\n",
    "            print('Tiempo Operación')\n",
    "            tiempo_operacion = self.LimpiarTiempoOperacion(datos_totales)\n",
    "            \n",
    "            \n",
    "            ##############################  LIMPIEZ DE FACT TIEMPO OPERACION\n",
    "            print('Limpiar fact')\n",
    "            fact_potencia_disponible = self.LimpiarFactPotenciaDisponible(datos_totales,agentes,tiempo_operacion)\n",
    "            \n",
    "            ################################### ALMACENAMIENTO DE DATOS\n",
    "            print('Almacenar Datos')\n",
    "            guardado = self.SaveData(fact_potencia_disponible)\n",
    "            \n",
    "            print(guardado)\n",
    "            print('Fin fecha: ' + str_fecha + ' al tiempo: ' + str(datetime.datetime.now()))   \n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def AgregarAgentes(self,df_datos_tiempos,unidad_negocio,centrales,unidades):\n",
    "        try:  \n",
    "            ##### DETALLE\n",
    "            df_datos = Refactorizar.AgregarDetalles(df_datos_tiempos,unidad_negocio,centrales,unidades)\n",
    "\n",
    "            return df_datos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def AgregarDatos(self,unidad_negocio,df_origen,df_pasos,df_total):\n",
    "        try:           \n",
    "            df_datos_totales = Refactorizar.AgregarAgtDistribucion(df_total,unidad_negocio)\n",
    "            \n",
    "            df_datos_totales = Refactorizar.AgregarAgtOrigen(df_datos_totales,unidad_negocio,df_origen)\n",
    "            \n",
    "            df_datos_totales = Refactorizar.AgregarPasos(df_datos_totales,df_pasos)\n",
    "\n",
    "            return df_datos_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    def LimpiarAgentes(self,datos):\n",
    "        try:\n",
    "            agentes_dw = Utilitarios\\\n",
    "            .ConvertPandasToSpark(self._genericDataFrame.spark,\n",
    "                                  self._fallasSniDA.GetAllAgtGen(),\n",
    "                                  Estructuras.Schema_Agt_Gen())\n",
    "            \n",
    "            udfNoVigenteUno = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR\\) ','',str(x)), StringType())\n",
    "            udfNoVigenteDos = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR \\)\\. ','',str(x)), StringType())\n",
    "            udfNoVigenteTres = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR\\)\\. ','',str(x)), StringType())\n",
    "            udfNoVigenteCuatro = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO _USAR\\) ','',str(x)), StringType())\n",
    "            udfNoVigenteCinco = UserDefinedFunction(lambda x: re.sub(' \\(NO VIGENTE. NO USAR\\) ','',str(x)), StringType())\n",
    "            udfNoVigenteSeis = UserDefinedFunction(lambda x: re.sub('NO VIGENTE. NO USAR\\). ','',str(x)), StringType())\n",
    "            \n",
    "            df_agentes = datos\\\n",
    "            .select(col('EmpresaCodigo').alias('agtevt_empresa_id_bk'),\n",
    "                    func.upper(col('Empresa')).alias('agtevt_empresa'),\n",
    "                    col('UNegocioCodigo').alias('agtevt_unidad_negocio_id_bk'),\n",
    "                    func.upper(regexp_replace(regexp_replace(col('UNegocio'),'CNEL EP ',''),'CELEC EP - ','')).alias('agtevt_unidad_negocio'),\n",
    "                    when(col('EstacionCodigo').isNull(),None).otherwise(col('EstacionCodigo')).alias('agtevt_central_id_bk'),\n",
    "                    when(col('Estacion').isNull(),None).otherwise(func.upper(regexp_replace(col('Estacion'),'CENTRAL ',''))).alias('agtevt_central'),\n",
    "                    col('ElementoCodigo').alias('agtevt_unidad_id_bk'),\n",
    "                    func.upper(col('Elemento')).alias('agtevt_unidad'),\n",
    "                    col('FechaInicioOpComercial').alias('agtevt_fecha_oper_comercial'),\n",
    "                    col('Version').alias('agtevt_version'),\n",
    "                    when(col('PotEfectiva').isNull(),0).otherwise(round(col('PotEfectiva').cast(FloatType()),2)).alias('agtevt_pot_efectiva')).distinct()\n",
    "            \n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteUno(column).alias(column) for column in df_agentes.columns])\n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteDos(column).alias(column) for column in df_agentes.columns])\n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteTres(column).alias(column) for column in df_agentes.columns])  \n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteCuatro(column).alias(column) for column in df_agentes.columns])\n",
    "            \n",
    "            df_agentes = Refactorizar.AsignarVersion(df_agentes,agentes_dw)\n",
    "            \n",
    "            self._df_agt_gen = df_agentes.exceptAll(agentes_dw.select(Dim_Agt_Gen.Schema()[1:]))\\\n",
    "            .withColumn('agtevt_id_pk',self._fallasSniDA.GetPkAgtGen() + \\\n",
    "                        func.row_number().over(Window.partitionBy().orderBy('agtevt_empresa_id_bk',\n",
    "                                                                            'agtevt_unidad_negocio_id_bk',\n",
    "                                                                            'agtevt_central_id_bk',\n",
    "                                                                            'agtevt_unidad_id_bk',\n",
    "                                                                            'agtevt_version')))\\\n",
    "            .select(Dim_Agt_Gen.Schema())\n",
    "            \n",
    "            df_agentes_totales = self._df_agt_gen.union(agentes_dw)\n",
    "            return df_agentes_totales            \n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "\n",
    "            \n",
    "    def LimpiarTiempoOperacion(self,datos):\n",
    "        try:\n",
    "            valueReplace = ['ENERO','FEBRERO','MARZO','ABRIL','MAYO','JUNIO','JULIO','AGOSTO','SEPTIEMBRE','OCTUBRE',\n",
    "                            'NOVIEMBRE','DICIEMBRE']        \n",
    "            paramReplace =  ['January','February','March','April','May','June','July','August','September','October',\n",
    "                             'November','December']\n",
    "            \n",
    "            df_fechas_dw = Utilitarios\\\n",
    "            .ConvertPandasToSpark(self._genericDataFrame.spark,\n",
    "                                  self._fallasSniDA.GetAllFechasTiempoOperacion(),\n",
    "                                  Estructuras.Schema_Tmp_Operacion())\n",
    "            \n",
    "            df_fechas = datos.select(to_timestamp('Fecha','yyyy-MM-dd HH:mm:ss').alias('Fecha')).distinct()\n",
    "            \n",
    "            df_fechas = df_fechas.select(func.substring(regexp_replace(regexp_replace(regexp_replace('Fecha', '-', ''),':',''),' ',''),0,10).alias('tmpop_id_pk').cast(IntegerType()),\n",
    "                                         col('Fecha').alias('tmpop_fecha'),\n",
    "                                         func.year('Fecha').alias('tmpop_anio').cast(ShortType()),\n",
    "                                         func.month('Fecha').alias('tmpop_mes_id').cast(ShortType()),\n",
    "                                         func.date_format(col('Fecha'), 'MMMMM').alias('tmpop_mes'),\n",
    "                                         func.date_format(col('Fecha'), 'dd').alias('tmpop_dia').cast(ShortType()),\n",
    "                                         func.hour('Fecha').alias('tmpop_hora').cast(ShortType()))\n",
    "            \n",
    "            df_fechas=df_fechas.na.replace(paramReplace,valueReplace,\"tmpop_mes\")\n",
    "            \n",
    "            #Conocemos si los registros estan almacenados para borrarlos del dataframe.\n",
    "            self._df_tiempo_operacion = df_fechas.exceptAll(df_fechas_dw)\n",
    "            df_fechas_totales = self._df_tiempo_operacion.union(df_fechas_dw)\n",
    "            return df_fechas_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    def LimpiarFactPotenciaDisponible(self,datos,agentes,tiempo_operacion):\n",
    "        try:            \n",
    "            #PK de producción\n",
    "            id_produccion = self._fallasSniDA.GetPkFactPotenciaDisponible()\n",
    "            \n",
    "            #Proceso de limpieza\n",
    "            df_fact = datos\\\n",
    "            .join(tiempo_operacion, \n",
    "                  (func.substring(regexp_replace(regexp_replace(regexp_replace(datos.Fecha, '-', ''),':',''),' ',''),0,10) == tiempo_operacion.tmpop_id_pk))\\\n",
    "            .join(agentes, \n",
    "                  (datos.EmpresaCodigo == agentes.agtevt_empresa_id_bk) & \\\n",
    "                  (datos.UNegocioCodigo == agentes.agtevt_unidad_negocio_id_bk) & \\\n",
    "                  (datos.EstacionCodigo == agentes.agtevt_central_id_bk) & \\\n",
    "                  (datos.ElementoCodigo == agentes.agtevt_unidad_id_bk) &\\\n",
    "                  (datos.Version == agentes.agtevt_version))\\\n",
    "            .select(agentes.agtevt_id_pk.alias('agtevt_id_fk'),\n",
    "                    tiempo_operacion.tmpop_id_pk.alias('tmpop_id_fk'),\n",
    "                    datos.PotenciaDisponible.alias('potdisp_pot_dispon').cast(FloatType()))\\\n",
    "            .withColumn('potdisp_id_pk', (id_produccion + func.row_number().over(Window.partitionBy()\\\n",
    "                                                                             .orderBy('agtevt_id_fk',\n",
    "                                                                                      'tmpop_id_fk'))).cast(LongType()))\\\n",
    "            .select(Fact_Pot_Disponible.Schema())\n",
    "                        \n",
    "            return df_fact\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def SaveData(self,df_datos):\n",
    "        \"\"\"Método para Guardar los datos limpios una vez procesados\"\"\"\n",
    "        try:\n",
    "            save = True\n",
    "            print('Lista de Agentes Generación: ' + str(datetime.datetime.now())) \n",
    "            save = save & self._fallasSniDA.Save(self._df_agt_gen,'cen_dws.dim_agt_gen',self._genericDataFrame)\n",
    "            \n",
    "            print('Lista de Fechas de Operación: ' + str(datetime.datetime.now()))\n",
    "            save = save & self._fallasSniDA.Save(self._df_tiempo_operacion,'cen_dws.dim_tmp_operacion',self._genericDataFrame)\n",
    "\n",
    "            print('Lista de datos: ' + str(datetime.datetime.now()))\n",
    "            save = save & self._fallasSniDA.Save(df_datos,'cen_dws.Fact_Pot_Disponible',self._genericDataFrame)\n",
    "\n",
    "            return save\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2020-07-22 13:08:50.847246\n",
      "Proceso de generación de datos: 2020-07-22 13:09:27.011298\n",
      "Limpiar Agentes\n",
      "Asignar Versiones\n",
      "Tiempo Operación\n",
      "Limpiar fact\n",
      "Almacenar Datos\n",
      "Lista de Agentes Generación: 2020-07-22 13:12:55.984647\n",
      "Lista de Fechas de Operación: 2020-07-22 13:16:17.191655\n",
      "Lista de datos: 2020-07-22 13:20:18.854875\n",
      "True\n",
      "Fin fecha: 1997-12-31 23:59 al tiempo: 2020-07-22 13:30:02.103513\n"
     ]
    }
   ],
   "source": [
    "proceso = TiempoOperacionBI()\n",
    "proceso.ProcesarDatos(2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
