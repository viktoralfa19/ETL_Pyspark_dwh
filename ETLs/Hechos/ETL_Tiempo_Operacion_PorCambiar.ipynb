{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "58eaaeb8-624c-42e0-bc3b-6cc7c47f47c0",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Transformación de tipo pandas a pyspark\n",
    "\n",
    "class Transformacion:\n",
    "    \"\"\"Clase para realizar la transformación de dataframe pandas a dataframe pyspark\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def equivalent_type(f):\n",
    "        if f == 'datetime64[ns]': return DateType()\n",
    "        elif f == 'int64': return LongType()\n",
    "        elif f == 'int32': return IntegerType()\n",
    "        elif f == 'float64': return FloatType()\n",
    "        else: return StringType()\n",
    "\n",
    "    def define_structure(self, string, format_type):\n",
    "        try: typo = self.equivalent_type(format_type)\n",
    "        except: typo = StringType()\n",
    "        return StructField(string, typo)\n",
    "\n",
    "\n",
    "    def pandas_to_spark(self, pandas_df, spark):\n",
    "        columns = list(pandas_df.columns)\n",
    "        types = list(pandas_df.dtypes)\n",
    "        struct_list = []\n",
    "        for column, typo in zip(columns, types): \n",
    "            struct_list.append(self.define_structure(column, typo))\n",
    "        p_schema = StructType(struct_list)\n",
    "        return spark.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "6224da63-f576-48b0-9fb9-b8d57cd03d3b",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Utilitarios\n",
    "\n",
    "class Utilitarios:\n",
    "    \"\"\"Clase para usar métodos bastante genéricos y en muchos casos estáticos\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def GenerateDataFileName(catalogo, fecha):\n",
    "        return 'file_'+catalogo+'_'+fecha\n",
    "    \n",
    "    @staticmethod\n",
    "    def GenerateCatalogueFileName(catalogo):\n",
    "        return \"file_\"+catalogo\n",
    "    \n",
    "    @staticmethod\n",
    "    def ConvertPandasToSpark(spark, dataframe_pandas, schema):\n",
    "        try:\n",
    "            if(dataframe_pandas is None):\n",
    "                return spark.createDataFrame([], schema)\n",
    "            else:\n",
    "                return spark.createDataFrame(dataframe_pandas, schema)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertSparkToPandas(dataframe_spark):\n",
    "        try:\n",
    "            if(dataframe_spark is None):\n",
    "                return None\n",
    "            return dataframe_spark.toPandas()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertStrListToSpark(spark, pd, str_list, list_column_names):\n",
    "        try:\n",
    "            if(str_list is None or len(str_list) == 0):\n",
    "                return None\n",
    "            return spark.createDataFrame((pd.DataFrame(str_list, columns = list_column_names)))\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def ConvertObjListToPandas(pd, obj_list, list_column_names):\n",
    "        try:\n",
    "            if(obj_list is None or len(obj_list) == 0):\n",
    "                return None\n",
    "            return pd.DataFrame(obj_list, columns = list_column_names)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "7fd3fd9a-7bc0-4283-ace1-7a76a7c848f0",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Manejo de Excepciones\n",
    "\n",
    "import sys, traceback\n",
    "\n",
    "class ExceptionManager:\n",
    "    \"\"\"Clase para manejar las excepciones.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Treatment(exception):\n",
    "        try:\n",
    "            print(exception)\n",
    "        except Py4JNetworkError as error:\n",
    "            print(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def TraceTreatment(exception):\n",
    "        try:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback)))\n",
    "            print(exception)\n",
    "        except Py4JNetworkError as error:\n",
    "            print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "33cf33aa-83ce-4f26-b0f4-ce76f309cdb5",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Configuracion de Contextos\n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "from sqlalchemy.orm import sessionmaker\n",
    "    \n",
    "class DBContextDw: \n",
    "        \"\"\"Clase que permite establecer la configuración con la bodega de datos o DataWarehouse.\"\"\"  \n",
    "                    \n",
    "        def __init__ (self):\n",
    "            self.HostDb= \"10.30.80.3\"\n",
    "            self.Port = \"5432\"\n",
    "            self.UserName = \"user_sirio\"\n",
    "            self.Password =\"Cen.2019.sirio\"\n",
    "            self.DataBase = \"dm_eventos\"\n",
    "            self.session_marker = None\n",
    "            \n",
    "        def Connection(self):\n",
    "            \"\"\"Método que permite obtener la sesión de conección a la base de datos\"\"\"\n",
    "            db_string = 'postgresql://{0}:{1}@{2}:{3}/{4}'.format(self.UserName, self.Password, self.HostDb,\n",
    "                                                                  self.Port, self.DataBase)\n",
    "            db = create_engine(db_string)  \n",
    "            self.session_maker = sessionmaker(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "azdata_cell_guid": "b8cf14e4-341f-484f-8a5f-fd9f6853825e",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Configuracion de Contextos\n",
    "\n",
    "class HDFSContext: \n",
    "    \"\"\"La clase ContextHdfs establece la configuración con el datalake para acceder a los diferentes archivos \n",
    "    almacenados de las bases de datos del HDFS.\"\"\"\n",
    "    \n",
    "    def __init__ (self, Host='10.30.80.3', Port='9000', Path='DATABASE/RDBMS', DataBase='SIVO', Schema='dbo'):\n",
    "        self.HostHdfs = Host\n",
    "        self.Port = Port\n",
    "        self.Path = Path\n",
    "        self.DataBase = DataBase\n",
    "        self.Schema = Schema\n",
    "        \n",
    "    def HdfsPath(self, tName, fName):        \n",
    "        \"\"\"Método que establece el path de búsqueda de un archivo específico.\"\"\"  \n",
    "        pathDir = \"hdfs://{0}:{1}/{2}/{3}/{4}/{5}/{6}\".format(self.HostHdfs, self.Port, self.Path, self.DataBase,\n",
    "                                                              self.Schema, tName, fName)\n",
    "        return pathDir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "azdata_cell_guid": "596cabc7-2e86-46ec-925c-9758f4d175b9",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Acceso a datos desde HDFS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class GenericDataFrame():\n",
    "    \"\"\"Clase para generar DataFrames Spark y trabajar en la lógica del ETL\"\"\"\n",
    "    def __init__ (self, hdfsContext):\n",
    "        self.hdfsContext = hdfsContext\n",
    "        self.url = 'jdbc:postgresql://10.30.80.3/dm_eventos'\n",
    "        self.properties = {'user': 'user_sirio', 'password': 'Cen.2019.sirio'}\n",
    "        self.modo = 'append'\n",
    "        self.spark = SparkSession.builder.appName(\"Sirio\")\\\n",
    "        .config('spark.driver.extraClassPath', '/home/jovyan/work/postgresql-42.2.12.jar').getOrCreate()#SparkSession.builder.appName(\"Sirio\").getOrCreate()\n",
    "    \n",
    "    def GetDataHdfs(self, tableName, fileName):\n",
    "        \"\"\"Método para retornar el DataFrame desde hadoop\"\"\"\n",
    "        path = self.hdfsContext.HdfsPath(tableName, fileName)\n",
    "        dataFrame = self.spark.read.json(path, multiLine=True)            \n",
    "        return dataFrame              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "azdata_cell_guid": "f494ff80-2c90-4d1c-9632-e9c9b670ffe6",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Creación del modelo\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base  \n",
    "from sqlalchemy import Column, String, SmallInteger, Integer, Numeric, BigInteger, DateTime, Boolean\n",
    "\n",
    "class Dim_Agt_Gen(declarative_base()):  \n",
    "    def __init__(self, pk, codigo_empresa, empresa, codigo_unegocio, unegocio, codigo_central, central\n",
    "                , codigo_unidad, unidad, fecha_operacion_comercial, version, pot_efectiva):\n",
    "        self.agtevt_id_pk = pk\n",
    "        self.agtevt_empresa_id_bk = codigo_empresa\n",
    "        self.agtevt_empresa = empresa\n",
    "        self.agtevt_unidad_negocio_id_bk = codigo_unegocio\n",
    "        self.agtevt_unidad_negocio = unegocio\n",
    "        self.agtevt_central_id_bk = codigo_central\n",
    "        self.agtevt_central = central\n",
    "        self.agtevt_unidad_id_bk = codigo_unidad\n",
    "        self.agtevt_unidad = unidad\n",
    "        self.agtevt_fecha_oper_comercial = fecha_operacion_comercial\n",
    "        self.agtevt_version = version\n",
    "        self.agtevt_pot_efectiva = pot_efectiva\n",
    "    \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['agtevt_id_pk', 'agtevt_empresa_id_bk', 'agtevt_empresa', 'agtevt_unidad_negocio_id_bk', 'agtevt_unidad_negocio'\n",
    "                , 'agtevt_central_id_bk', 'agtevt_central', 'agtevt_unidad_id_bk', 'agtevt_unidad', 'agtevt_fecha_oper_comercial'\n",
    "                , 'agtevt_version', 'agtevt_pot_efectiva']\n",
    "        \n",
    "    __tablename__ = 'dim_agt_gen'\n",
    "    __table_args__ = {'schema' : 'cen_dws'} \n",
    "\n",
    "    agtevt_id_pk = Column(Integer, primary_key=True)\n",
    "    agtevt_empresa_id_bk = Column(String)\n",
    "    agtevt_empresa = Column(String)\n",
    "    agtevt_unidad_negocio_id_bk = Column(String)\n",
    "    agtevt_unidad_negocio = Column(String)\n",
    "    agtevt_central_id_bk = Column(String)\n",
    "    agtevt_central = Column(String)\n",
    "    agtevt_unidad_id_bk = Column(String)\n",
    "    agtevt_unidad = Column(String)\n",
    "    agtevt_fecha_oper_comercial = Column(DateTime)\n",
    "    agtevt_version = Column(SmallInteger)\n",
    "    agtevt_pot_efectiva = Column(Numeric(10,2))\n",
    "    \n",
    "    \n",
    "class Dim_Tmp_Operacion(declarative_base()):  \n",
    "    def __init__(self, pk, fecha, anio, id_mes, mes, dia, hora):\n",
    "        self.tmpop_id_pk = pk\n",
    "        self.tmpop_fecha = fecha\n",
    "        self.tmpop_anio = anio\n",
    "        self.tmpop_mes_id = id_mes\n",
    "        self.tmpop_mes = mes\n",
    "        self.tmpop_dia = dia\n",
    "        self.tmpop_hora = hora\n",
    "      \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['tmpop_id_pk', 'tmpop_fecha', 'tmpop_anio', 'tmpop_mes_id', 'tmpop_mes', 'tmpop_dia', 'tmpop_hora']\n",
    "    \n",
    "    __tablename__ = 'dim_tmp_operacion'\n",
    "    __table_args__ = {'schema' : 'cen_dws'}\n",
    "\n",
    "    tmpop_id_pk = Column(Integer, primary_key=True)\n",
    "    tmpop_fecha = Column(DateTime)\n",
    "    tmpop_anio = Column(SmallInteger)    \n",
    "    tmpop_mes_id = Column(SmallInteger)\n",
    "    tmpop_mes = Column(String)\n",
    "    tmpop_dia = Column(SmallInteger)\n",
    "    tmpop_hora = Column(SmallInteger)\n",
    "      \n",
    "    \n",
    "class Fact_Tiempo_Operacion(declarative_base()): \n",
    "    \"\"\"Módelo de la tabla de hechos\"\"\"\n",
    "    def __init__(self, pk, agente_gen, tmpop, tiempo_disponible, tiempo_indisponible):\n",
    "        self.evt_id_pk = pk\n",
    "        self.agtevt_id_fk = agente_gen\n",
    "        self.tmpop_id_fk = tmpop\n",
    "        self.evt_tiempo_disponible = tiempo_disponible\n",
    "        self.evt_tiempo_indisponible = tiempo_indisponible\n",
    "        \n",
    "    @classmethod\n",
    "    def Schema(cls):\n",
    "        return ['evt_id_pk', 'agtevt_id_fk', 'tmpop_id_fk', 'evt_tiempo_disponible', 'evt_tiempo_indisponible']\n",
    "    \n",
    "    __tablename__ = 'fact_tiempo_operacion'\n",
    "    __table_args__ = {'schema' : 'cen_dws'}\n",
    "\n",
    "    evt_id_pk = Column(BigInteger, primary_key=True) \n",
    "    agtevt_id_fk = Column(Integer)\n",
    "    tmpop_id_fk = Column(Integer)\n",
    "    evt_tiempo_disponible = Column(Numeric(10, 2))\n",
    "    evt_tiempo_indisponible = Column(Numeric(10, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "7cec5f13-2317-44ad-8811-5c05758d8d27",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Estructuras de Datos Esquemas\n",
    "\n",
    "class Estructuras:\n",
    "    \"\"\"Clase para obtener de manera estática las estructuras de los dataframes\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Agt_Gen():\n",
    "        schema = StructType([StructField('agtevt_id_pk', IntegerType(), False),\n",
    "                             StructField('agtevt_empresa_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_empresa', StringType(), False),\n",
    "                             StructField('agtevt_unidad_negocio_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_unidad_negocio', StringType(), False),\n",
    "                             StructField('agtevt_central_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_central', StringType(), False),\n",
    "                             StructField('agtevt_unidad_id_bk', StringType(), False),\n",
    "                             StructField('agtevt_unidad', StringType(), False),\n",
    "                             StructField('agtevt_fecha_oper_comercial', TimestampType(), False),\n",
    "                             StructField('agtevt_version', ShortType(), False),\n",
    "                             StructField('agtevt_pot_efectiva', FloatType(), False)\n",
    "                             ])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Tmp_Operacion():\n",
    "        schema = StructType([StructField('tmpop_id_pk', IntegerType(), False),\n",
    "                             StructField('tmpop_fecha', TimestampType(), False),\n",
    "                             StructField('tmpop_anio', ShortType(), False),\n",
    "                             StructField('tmpop_mes_id', ShortType(), False),\n",
    "                             StructField('tmpop_mes', StringType(), False),\n",
    "                             StructField('tmpop_dia', ShortType(), False),\n",
    "                             StructField('tmpop_hora', ShortType(), False)\n",
    "                             ])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_Fact_Tiempo_Operacion():\n",
    "        schema = StructType([StructField('evt_id_pk', LongType(), False),\n",
    "                             StructField('agtevt_id_fk', IntegerType(), False),\n",
    "                             StructField('tmpop_id_fk', IntegerType(), False),\n",
    "                             StructField('evt_tiempo_disponible', FloatType(), True),\n",
    "                             StructField('evt_tiempo_indisponible', FloatType(), True)\n",
    "                             ])\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estructuras de Datos Esquemas\n",
    "\n",
    "class EstructurasHDFS:\n",
    "    \"\"\"Clase para obtener de manera estática las estructuras de los dataframes\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_EVENTO():\n",
    "        schema = StructType([StructField('EVENTO_ID', LongType(), True),\n",
    "                             StructField('EVENTO_FECHA', TimestampType(), True),\n",
    "                             StructField('EVENTO_CLASE', ShortType(), True),\n",
    "                             StructField('TPB_EVENTO_ID', ShortType(), True),\n",
    "                             StructField('TPB_CALIF_ID', ShortType(), True),\n",
    "                             StructField('TPB_CLASIF_CALIF_ID', ShortType(), True),\n",
    "                             StructField('TPB_CATEG_CLASIF_ID', ShortType(), True),\n",
    "                             StructField('EVENTO_NUM_REDSP', ShortType(), True),\n",
    "                             StructField('NIVELVOLTAJE_ID', ShortType(), True),\n",
    "                             StructField('EMPRESA_ID', ShortType(), True),\n",
    "                             StructField('ESTACION_ID', ShortType(), True),\n",
    "                             StructField('SAM_ID', IntegerType(), True),\n",
    "                             StructField('SAF_ID', IntegerType(), True),\n",
    "                             StructField('SISTEMA_CLASE', ShortType(), True),\n",
    "                             StructField('USUARIO_ID_CREADOR', ShortType(), True),\n",
    "                             StructField('USUARIO_ID_MODIFICADOR', ShortType(), True),\n",
    "                             StructField('EVENTO_FECHACREACION', TimestampType(), True),\n",
    "                             StructField('EVENTO_FECHAMODIFICACION', TimestampType(), True),\n",
    "                             StructField('NOMBRE_USUARIO_INGRESO', StringType(), True),\n",
    "                             StructField('EVENTO_ES_RELEVANTE', BooleanType(), True),\n",
    "                             StructField('ANIO', ShortType(), True)])\n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def Schema_EVENTO_DTL():\n",
    "        schema = StructType([StructField('EVENTO_ID', LongType(), True),\n",
    "                             StructField('EVENTO_FECHA', TimestampType(), True),\n",
    "                             StructField('EVENTO_DTL_ID', ShortType(), True),\n",
    "                             StructField('EVENTO_DTL_PODER', FloatType(), True),\n",
    "                             StructField('EVENTO_DTL_NOTA', StringType(), True),\n",
    "                             StructField('EVENTO_DTL_ULTIMO', ShortType(), True),\n",
    "                             StructField('ANIO', ShortType(), True)])\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "12680cac-7be4-464a-b092-d15984339c44",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Acceso a Datos\n",
    "\n",
    "from sqlalchemy.sql.expression import func as alchemy_func\n",
    "from sqlalchemy import exc\n",
    "import pandas as pd\n",
    "\n",
    "class TiempoOperacionDA:\n",
    "    \"\"\"Clase para realizar el acceso a datos y persistencia de información de Tiempo Operacion\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dBContextDw = DBContextDw()\n",
    "        self._dBContextDw.Connection()\n",
    "        self.session_maker = self._dBContextDw.session_maker\n",
    "\n",
    "    def GetPkAgtGen(self):\n",
    "        \"\"\"Método para obtener el Id máximo de la tabla\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            max_pk = session.query(alchemy_func.max(Dim_Agt_Gen.agtevt_id_pk)).scalar()\n",
    "            if(max_pk is None):\n",
    "                max_pk = 0\n",
    "            return max_pk\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "            \n",
    "    def GetPkFactTiempoOperacion(self):\n",
    "        \"\"\"Método para obtener el Id máximo de la tabla de hechos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            max_pk = session.query(alchemy_func.max(Fact_Tiempo_Operacion.evt_id_pk)).scalar()\n",
    "            if(max_pk is None):\n",
    "                max_pk = 0\n",
    "            return max_pk\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetFechaHoraMaximaTiempoOperacion(self):\n",
    "        \"\"\"Método para obtener la fecha máxima de datos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            \n",
    "            max_date = session.query(alchemy_func.max(Fact_Tiempo_Operacion.tmpop_id_fk)).scalar()\n",
    "            registro = session.query(Dim_Tmp_Operacion).filter(Dim_Tmp_Operacion.tmpop_id_pk == max_date).first()\n",
    "            \n",
    "            if(registro is None):\n",
    "                fecha = '1997-12-31 23:59'\n",
    "            else:\n",
    "                fecha = str(registro.tmpop_fecha)\n",
    "            return fecha\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetAllFechasTiempoOperacion(self, anio=None):\n",
    "        \"\"\"Método para obtener todos todos los datos de la demensión de tiempo\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            if(anio == None):\n",
    "                query = session.query(Dim_Tmp_Operacion) \n",
    "            else:\n",
    "                query = session.query(Dim_Tmp_Operacion).filter(Dim_Tmp_Operacion.tmpop_anio == anio)\n",
    "            df = pd.read_sql(query.statement, query.session.bind)\n",
    "            return df\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def GetAllAgtGen(self):\n",
    "        \"\"\"Método para obtener todos todos los datos\"\"\"\n",
    "        try:\n",
    "            session = self.session_maker()\n",
    "            query = session.query(Dim_Agt_Gen) \n",
    "            df = pd.read_sql(query.statement, query.session.bind)\n",
    "            return df\n",
    "        except exc.SQLAlchemyError as error: \n",
    "            ExceptionManager.Treatment(error)\n",
    "            raise \n",
    "        finally:\n",
    "            session.close()\n",
    "            \n",
    "    def Save(self, dataframe, nombre_tabla, generic_dataframe):\n",
    "        \"\"\"Método para almacenar las dimensiones y la tabla de hechos en el DW\"\"\"\n",
    "        try:\n",
    "            dataframe.write.jdbc(url=generic_dataframe.url, table=nombre_tabla, mode = generic_dataframe.modo, properties=generic_dataframe.properties)\n",
    "            return True\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "a5e4bc50-cdb9-4e36-bda7-31edcdd89119",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "## Clases adicionales para refcatorizar codigo\n",
    "\n",
    "class Refactorizar:\n",
    "    \"\"\"Contiene metodos auxiliares del negocio\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def DafaFrameUnidadNegocio(df_empresa,df_cat_empresa,df_cat_unidad_negocio):\n",
    "        try:\n",
    "            df_unidad_negocio = df_empresa.select(col('EMPRESA_ID').alias('UNegocioId'),\n",
    "                                                  col('EMPRESA_CODIGO').alias('UNegocioCodigo'),\n",
    "                                                  col('EMPRESA_NOMBRE').alias('UNegocio'),\n",
    "                                                  col('EMPRESA_CODIGO').alias('EmpresaCodigo'),\n",
    "                                                  col('EMPRESA_NOMBRE').alias('Empresa'))\n",
    "            \n",
    "            df_cat_unidad_negocio = df_cat_unidad_negocio\\\n",
    "            .join(df_cat_empresa, df_cat_unidad_negocio.IdEmpresa == df_cat_empresa.IdEmpresa)\\\n",
    "            .select(df_cat_unidad_negocio.IdUNegocio.alias('UNegocioId'),\n",
    "                    df_cat_unidad_negocio.Codigo.alias('UNegocioCodigo'),\n",
    "                    df_cat_unidad_negocio.Nombre.alias('UNegocio'),\n",
    "                    df_cat_empresa.Codigo.alias('EmpresaCodigo'),\n",
    "                    df_cat_empresa.Nombre.alias('Empresa'))\n",
    "            \n",
    "            return df_cat_unidad_negocio.union(df_unidad_negocio).distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def DafaFrameEstacion(df_central,df_cat_central):\n",
    "        try:\n",
    "            df_central = df_central.select(col('ESTACION_ID').alias('EstacionId'),\n",
    "                                           col('ESTACION_CODIGO').alias('EstacionCodigo'),\n",
    "                                           col('ESTACION_NOMBRE').alias('Estacion'),\n",
    "                                           col('ESTACION_ID').alias('EstacionPadreId'))\n",
    "            \n",
    "            df_cat_central = df_cat_central.select(col('IdCentral').alias('EstacionId'),\n",
    "                                                   col('Codigo').alias('EstacionCodigo'),\n",
    "                                                   col('Nombre').alias('Estacion'),\n",
    "                                                   col('IdCentral').alias('EstacionPadreId'))\n",
    "            \n",
    "            df_cat_central = df_cat_central.union(df_central).distinct()\n",
    "\n",
    "            return df_cat_central.distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def DafaFrameElemento(df_unidad,df_cat_unidad):\n",
    "        try:\n",
    "            df_unidad = df_unidad.select(col('ELEMENTO_ID').alias('ElementoId'),\n",
    "                                         col('ELEMENTO_CODIGO').alias('ElementoCodigo'),\n",
    "                                         col('ELEMENTO_NOMBRE').alias('Elemento'),\n",
    "                                         col('ELEMENTO_TIPO').alias('Tipo'),\n",
    "                                         lit(0).alias('VoltajeId'),\n",
    "                                         col('EMPRESA_ID').alias('UNegocioId'),\n",
    "                                         col('ESTACION_ID').alias('EstacionId'),\n",
    "                                         to_timestamp(when(col('UNIDAD_FECHAOPERACION').isNull(),col('ELEMENTO_FECHACREACION'))\\\n",
    "                                         .otherwise(col('UNIDAD_FECHAOPERACION')),'yyyy-MM-dd HH:mm')\\\n",
    "                                         .alias('FechaInicioOpComercial'),\n",
    "                                         col('ELEMENTO_FECHACREACION').alias('FechaCreacion'),\n",
    "                                         col('UNIDAD_POTENCIAEFECTIVA').alias('PotEfectiva'))\n",
    "            \n",
    "            df_cat_unidad = df_cat_unidad.select(col('IdUnidad').alias('ElementoId'),\n",
    "                                                 col('Codigo').alias('ElementoCodigo'),\n",
    "                                                 col('Nombre').alias('Elemento'),\n",
    "                                                 lit(1).alias('Tipo'),\n",
    "                                                 lit(0).alias('VoltajeId'),\n",
    "                                                 col('IdUNegocio').alias('UNegocioId'),\n",
    "                                                 col('IdCentral').alias('EstacionId'),\n",
    "                                                 to_timestamp(col('FechaInicioOpComercial'),'yyyy-MM-dd HH:mm')\\\n",
    "                                                 .alias('FechaInicioOpComercial'),\n",
    "                                                 col('FechaCreacion'),\n",
    "                                                 col('Pot_Efectiva').alias('PotEfectiva'))\n",
    "            \n",
    "            df_cat_unidad = df_cat_unidad.union(df_unidad).distinct()\n",
    "            \n",
    "            return df_cat_unidad.distinct()\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def AgregarDetalles(df_datos,unidad_negocio,centrales,unidades):\n",
    "        try:\n",
    "            df_datos = df_datos\\\n",
    "            .join(unidad_negocio, df_datos.UNegocioId == unidad_negocio.UNegocioId)\\\n",
    "            .join(centrales, df_datos.CentralId == centrales.EstacionId)\\\n",
    "            .join(unidades, df_datos.UnidadId == unidades.ElementoId)\\\n",
    "            .select(unidad_negocio.UNegocioCodigo,\n",
    "                    unidad_negocio.UNegocio,\n",
    "                    unidad_negocio.EmpresaCodigo,\n",
    "                    unidad_negocio.Empresa,\n",
    "                    centrales.EstacionCodigo,\n",
    "                    centrales.Estacion,\n",
    "                    unidades.ElementoCodigo,\n",
    "                    unidades.Elemento,\n",
    "                    unidades.FechaInicioOpComercial,\n",
    "                    unidades.PotEfectiva,\n",
    "                    lit(1).alias('Version'),\n",
    "                    df_datos.Fecha,\n",
    "                    df_datos.TiempoOperacion).distinct()  \n",
    "            return df_datos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "               \n",
    "    @staticmethod\n",
    "    def AsignarVersion(df_agentes,agentes_dw):\n",
    "        try:\n",
    "            ##### VALIDAR LA VERSIÓN PARA LOS AGENTES DE GENERACIÓN\n",
    "            agt_gen = df_agentes\n",
    "            agt_gen_dw = agentes_dw\n",
    "            \n",
    "            agt_new_version = agt_gen.join(agt_gen_dw,\n",
    "                                           (agt_gen.agtevt_empresa_id_bk == agt_gen_dw.agtevt_empresa_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_unidad_negocio_id_bk == agt_gen_dw.agtevt_unidad_negocio_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_central_id_bk == agt_gen_dw.agtevt_central_id_bk) &\\\n",
    "                                           (agt_gen.agtevt_unidad_id_bk == agt_gen_dw.agtevt_unidad_id_bk))\\\n",
    "            .filter(agt_gen.agtevt_pot_efectiva != agt_gen_dw.agtevt_pot_efectiva)\\\n",
    "            .groupby(agt_gen.agtevt_empresa_id_bk,agt_gen.agtevt_unidad_negocio_id_bk,\n",
    "                     agt_gen.agtevt_central_id_bk,agt_gen.agtevt_unidad_id_bk)\\\n",
    "            .agg(func.max(agt_gen.agtevt_version).alias('agtevt_version'))\n",
    "            \n",
    "            ##### SUMAMOS UNO A LA VERSIÓN DE LOS AGENTES QUE HAN CAMBIADO SU POTENCIA EFECTIVA\n",
    "            df_agentes = df_agentes.join(agt_new_version,\n",
    "                                        (agt_new_version.agtevt_empresa_id_bk == df_agentes.agtevt_empresa_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_unidad_negocio_id_bk == df_agentes.agtevt_unidad_negocio_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_central_id_bk == df_agentes.agtevt_central_id_bk) &\\\n",
    "                                        (agt_new_version.agtevt_unidad_id_bk == df_agentes.agtevt_unidad_id_bk), how='left')\\\n",
    "            .select(df_agentes.agtevt_empresa_id_bk,\n",
    "                    df_agentes.agtevt_empresa,\n",
    "                    df_agentes.agtevt_unidad_negocio_id_bk,\n",
    "                    df_agentes.agtevt_unidad_negocio,\n",
    "                    df_agentes.agtevt_central_id_bk,\n",
    "                    df_agentes.agtevt_central,\n",
    "                    df_agentes.agtevt_unidad_id_bk,\n",
    "                    df_agentes.agtevt_unidad,\n",
    "                    df_agentes.agtevt_fecha_oper_comercial.cast(TimestampType()),\n",
    "                    when(agt_new_version.agtevt_version.isNull(),df_agentes.agtevt_version)\\\n",
    "                    .otherwise(agt_new_version.agtevt_version+1).alias('agtevt_version').cast(ShortType()),\n",
    "                    round(df_agentes.agtevt_pot_efectiva.cast(FloatType()),2).alias('agtevt_pot_efectiva'))\n",
    "\n",
    "            return df_agentes\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def AsignarVersionesNuevas(datos_totales,df_agente_gen):\n",
    "        try:\n",
    "            datos_totales = datos_totales\\\n",
    "            .join(df_agente_gen,\n",
    "                  (datos_totales.EmpresaCodigo == df_agente_gen.agtevt_empresa_id_bk) &\\\n",
    "                  (datos_totales.UNegocioCodigo == df_agente_gen.agtevt_unidad_negocio_id_bk) &\\\n",
    "                  (datos_totales.EstacionCodigo == df_agente_gen.agtevt_central_id_bk) &\\\n",
    "                  (datos_totales.ElementoCodigo == df_agente_gen.agtevt_unidad_id_bk) &\\\n",
    "                  (datos_totales.PotEfectiva == df_agente_gen.agtevt_pot_efectiva), how='left')\\\n",
    "            .select(datos_totales.EmpresaCodigo,\n",
    "                    datos_totales.Empresa,\n",
    "                    datos_totales.UNegocioCodigo,\n",
    "                    datos_totales.UNegocio,\n",
    "                    datos_totales.EstacionCodigo,\n",
    "                    datos_totales.Estacion,\n",
    "                    datos_totales.ElementoCodigo,\n",
    "                    datos_totales.Elemento,\n",
    "                    datos_totales.PotEfectiva,\n",
    "                    when(df_agente_gen.agtevt_pot_efectiva.isNull(),datos_totales.Version)\\\n",
    "                    .otherwise(df_agente_gen.agtevt_version).alias('Version'),\n",
    "                    datos_totales.Fecha,\n",
    "                    datos_totales.TiempoOperacion)\n",
    "            \n",
    "            return datos_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "        \n",
    "    @staticmethod\n",
    "    def ConstruirHorasAnuales(spark,anio,unidades):\n",
    "        try:\n",
    "            if(anio==None):\n",
    "                anioInicio=1998\n",
    "                anioFin=datetime.datetime.now().year\n",
    "            else:\n",
    "                anioInicio=anio\n",
    "                anioFin=anio\n",
    "                \n",
    "            #fechaInicio = str(anioInicio)+'-01-01 00:00:00'\n",
    "            #fechaFin = str(anioFin)+'-12-31 23:59:59'\n",
    "            \n",
    "            fechaInicio = str(anioInicio)+'-10-04 00:00:00'\n",
    "            fechaFin = str(anioFin)+'-10-04 23:59:59'\n",
    "            \n",
    "            df_time = spark\\\n",
    "            .sql(\"SELECT sequence(to_timestamp('{}'), to_timestamp('{}'), interval 1 hour) as timestamp\"\\\n",
    "                 .format(fechaInicio,fechaFin))\n",
    "            df_time = df_time.withColumn(\"Fecha\", explode(col(\"timestamp\"))).select('Fecha')\n",
    "            df_time = df_time.withColumn('Id',1+func.row_number().over(Window.partitionBy().orderBy('Fecha')))\n",
    "\n",
    "            df_time_par = df_time.filter(df_time.Id%2==0).select(col('Fecha').alias('FechaPar'),col('Id').alias('IdPar'))\n",
    "            df_time_impar = df_time.filter(df_time.Id%2!=0).select(col('Fecha').alias('FechaImpar'),col('Id').alias('IdImpar'))\n",
    "\n",
    "\n",
    "            df_time_total_ida = df_time_par.join(df_time_impar, (df_time_par.IdPar-1)==df_time_impar.IdImpar)\\\n",
    "            .select(df_time_impar.FechaImpar.alias('FechaInicio'),df_time_par.FechaPar.alias('FechaFin'))\n",
    "\n",
    "            df_time_total_vuelta = df_time_impar.join(df_time_par, (df_time_par.IdPar+1)==df_time_impar.IdImpar)\\\n",
    "            .select(df_time_par.FechaPar.alias('FechaInicio'),df_time_impar.FechaImpar.alias('FechaFin'))\n",
    "\n",
    "            df_time_total = df_time_total_ida.union(df_time_total_vuelta).orderBy('FechaInicio')\n",
    "        \n",
    "            rdd = df_time_total.rdd.cartesian(unidades.rdd).map(lambda x: (x[1][0],x[0][0],x[0][1]))\n",
    "            df_total = rdd.toDF(['IdUnidad','FechaInicio','FechaFin'])\n",
    "            return df_total\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def RellenarHoras(fechaInicio,fechaAuxiliar,estado,FechaFinAnio):\n",
    "        try:\n",
    "            DatosTotales = []\n",
    "            while (fechaInicio<fechaAuxiliar):\n",
    "                minuto=0\n",
    "                if(estado is False):\n",
    "                    minuto=60\n",
    "                DatosTotales.append([fechaInicio,'None',minuto,not estado])\n",
    "                fechaInicio += timedelta(hours=1)\n",
    "            \n",
    "            if(estado):\n",
    "                minutoAlFinalHora=60-fechaAuxiliar.minute\n",
    "            else:\n",
    "                minutoAlFinalHora=fechaAuxiliar.minute\n",
    "            \n",
    "            if(fechaAuxiliar < FechaFinAnio):\n",
    "                DatosTotales[-1][1]=fechaAuxiliar.strftime(\"%Y-%m-%d %H:%M\")\n",
    "                DatosTotales[-1][2]=minutoAlFinalHora\n",
    "                DatosTotales[-1][3]=estado\n",
    "            return DatosTotales\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def RecorrerDatos(x):\n",
    "        try:\n",
    "            id = x[0]\n",
    "            datos = list(x[1])\n",
    "            DatosTotales = []\n",
    "            anio = datos[0][0].year\n",
    "            estadoInicial = datos[0][1] #EL - true, FL - false\n",
    "            estadoFin = not datos[-1][1]\n",
    "            fechaFin = datos[0][0]\n",
    "\n",
    "            fechaInicioAnio = datetime.datetime(anio,1,1,0,0,0)\n",
    "            fechaFinAnio = datetime.datetime(anio+1,1,1,0,0,0)\n",
    "\n",
    "            datos.append([fechaFinAnio,estadoFin])\n",
    "\n",
    "            conjuntoDatos = Refactorizar.RellenarHoras(fechaInicioAnio,fechaFin,estadoInicial,fechaFinAnio)\n",
    "            DatosTotales += conjuntoDatos\n",
    "\n",
    "            misma_hora=None\n",
    "            for i in range(0,len(datos)-1):\n",
    "                fechaInicio = datos[i][0]\n",
    "                fechaInicio -= timedelta(minutes=fechaInicio.minute)\n",
    "                fechaFin = datos[i+1][0]   \n",
    "                estado = datos[i+1][1]     \n",
    "                #print(DatosTotales)\n",
    "                if(fechaInicio.hour!=fechaFin.hour):\n",
    "                    conjuntoDatos = Refactorizar.RellenarHoras(fechaInicio,fechaFin,estado,fechaFinAnio)\n",
    "                    if(misma_hora==None):\n",
    "                        DatosTotales += conjuntoDatos[1:]\n",
    "                    else:\n",
    "                        DatosTotales = DatosTotales[:-1]\n",
    "                        dato = Refactorizar.CalcularMismaHora(misma_hora)\n",
    "                        DatosTotales += dato\n",
    "                        misma_hora=None\n",
    "                        DatosTotales += conjuntoDatos[1:]\n",
    "                else:\n",
    "                    if(misma_hora==None):\n",
    "                        misma_hora=[datos[i][1],[fechaInicio,datos[i][0],datos[i+1][0]]]\n",
    "                    else:\n",
    "                        misma_hora[1].append(datos[i+1][0])\n",
    "\n",
    "\n",
    "            #estadoInicial = not datos[-1][1]\n",
    "            #fechaInicio = datos[-1][0]\n",
    "            #fechaInicio -= timedelta(minutes=fechaInicio.minute)\n",
    "            #conjuntoDatos = RellenarHoras(fechaInicio,fechaFinAnio,estadoInicial,fechaFinAnio)\n",
    "            #DatosTotales += conjuntoDatos[1:]\n",
    "\n",
    "            return (id,DatosTotales)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "                \n",
    "    def CalcularMismaHora(horas):\n",
    "        total=0\n",
    "        estadoInicial=not horas[0]\n",
    "        #if(not estadoInicial):\n",
    "        finHora= horas[1][0] +  timedelta(hours=1)\n",
    "        horas[1].append(finHora)\n",
    "\n",
    "        for i in range(0,len(horas[1])-1):\n",
    "            inicio=horas[1][i]\n",
    "            fin=horas[1][i+1]\n",
    "            dif_minutos = divmod((fin-inicio).total_seconds(), 60)[0]\n",
    "\n",
    "            if(not estadoInicial and (i+1)%2==0):\n",
    "                total += dif_minutos\n",
    "            if(estadoInicial and (i+1)%2==1):\n",
    "                total += dif_minutos\n",
    "        return [[horas[1][0],horas[1][1].strftime('%Y-%m-%d %H:%M'),total,not estadoInicial]]\n",
    "     \n",
    "        \n",
    "    def CalcularHorasIntermedias(x):\n",
    "        try:\n",
    "            id=x[0]\n",
    "            datos=list(x[1])\n",
    "\n",
    "            datosProcesamiento = []\n",
    "            datosSalida = []\n",
    "            fechaRegistroInicial = datos[0][0]\n",
    "            estadoRegistroInicial = datos[0][1]\n",
    "            fechaCreacionInicial = datos[0][2]\n",
    "\n",
    "            #Primero si los minutos del primer registro es diferente de cero entonces:\n",
    "            #Agrego el limiter inferior con hora y minuto cero y con estado contrario al primer registro\n",
    "            if(fechaRegistroInicial.minute!=0):\n",
    "                fechaInicial = fechaRegistroInicial - timedelta(minutes=fechaRegistroInicial.minute)\n",
    "                estadoInicial = not estadoRegistroInicial\n",
    "                datosProcesamiento.append([fechaInicial,estadoInicial,fechaCreacionInicial])\n",
    "\n",
    "            #Agrego la data ingresada\n",
    "            datosProcesamiento += datos\n",
    "\n",
    "            #Siempre agrego el limite superior con la hora y minuto en cero (la siguiente hora),\n",
    "            #con estado cambiado del ultimo registro\n",
    "            fechaRegistroFinal = datos[-1][0]\n",
    "            estadoRegistroFinal = datos[-1][1]\n",
    "            fechaCreacionFinal = datos[-1][2]\n",
    "\n",
    "            fechaFinal = (fechaRegistroFinal - timedelta(minutes=fechaRegistroFinal.minute)) + timedelta(hours=1)\n",
    "            estadoFinal = not estadoRegistroFinal\n",
    "            datosProcesamiento.append([fechaFinal,estadoFinal,fechaCreacionFinal])\n",
    "\n",
    "            #Iteramos, tomo el elemento i-1 y el elemento i, i el estado i es falso, el tiempo es la diferencia,\n",
    "            #caso contrario el tiempo es cero y saco el registro i\n",
    "            for i in range(0,len(datosProcesamiento)):\n",
    "                if(i==0):\n",
    "                    fecha = datosProcesamiento[i][0]\n",
    "                    estado = datosProcesamiento[i][1]\n",
    "                    fechaCreacion = datosProcesamiento[i][2]\n",
    "                    tiempo = 0.0\n",
    "                    datosSalida.append([fecha,tiempo,estado,fechaCreacion])\n",
    "                    continue\n",
    "\n",
    "                datoAnterior = datosProcesamiento[i-1]\n",
    "                datoActual = datosProcesamiento[i]\n",
    "                estadoActual = datoActual[1]\n",
    "                fechaCreacionActual = datoActual[2]\n",
    "                fechaAnterior = datoAnterior[0]\n",
    "                fechaActual = datoActual[0]\n",
    "                if(estadoActual is False):\n",
    "                    tiempo = divmod((fechaActual-fechaAnterior).total_seconds(), 60)[0]\n",
    "                    datosSalida.append([fechaActual,tiempo,estadoActual,fechaCreacionActual])\n",
    "                else:\n",
    "                    tiempo = 0.0\n",
    "                    datosSalida.append([fechaActual,tiempo,estadoActual,fechaCreacionActual])\n",
    "\n",
    "            #Sumo el tiempo del último registro al tiempo del penultimo registro\n",
    "            datosSalida[-2][1] += datosSalida[-1][1]\n",
    "\n",
    "            #si los datos de salida tienen dos elementos, devuelvo el primer registro, caso contrario devuelvo\n",
    "            #los datos desde el segundo registro al penultimo registro\n",
    "            if(len(datosSalida)==2):\n",
    "                datosSalida = datosSalida[0:1]\n",
    "            else:\n",
    "                datosSalida = datosSalida[1:-1]   \n",
    "            return (id,datosSalida)\n",
    "        except Exception as error:\n",
    "                ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def RellenarFechas(x):\n",
    "        try:\n",
    "            id = x[0]\n",
    "            datos = list(x[1])\n",
    "\n",
    "            anio = datos[0][0].year\n",
    "            fechaCreacion = datos[0][3]\n",
    "            datosSalida = []\n",
    "            fechaInicioAnio = datetime.datetime(anio,1,1,0,0,0)\n",
    "            fechaInicioAnioSiguiente = datetime.datetime(anio+1,1,1,0,0,0)\n",
    "\n",
    "            #Validamos: 1.- Si el es el primero recorremos e insertamos las horas desde el registro 1 hasta el inicio del año\n",
    "            for i in range(0,len(datos)):\n",
    "                tiempo = 0.0\n",
    "                if(i == 0):\n",
    "                    fechaLimiteInferior = datos[i][0] - (timedelta(minutes=datos[i][0].minute) + timedelta(hours=1))\n",
    "                    estadoActual = datos[i][2]\n",
    "                    if(not estadoActual):\n",
    "                        tiempo = 60.0\n",
    "                    if(fechaCreacion>fechaInicioAnio):\n",
    "                        fechaInicioAnio = fechaCreacion\n",
    "                    if(fechaLimiteInferior>fechaInicioAnio):\n",
    "                        while(fechaLimiteInferior>=fechaInicioAnio):\n",
    "                            #print([fechaLimiteInferior,tiempo,not estadoActual])\n",
    "                            datosSalida.append([fechaLimiteInferior,tiempo,not estadoActual,fechaCreacion])\n",
    "                            fechaLimiteInferior -= timedelta(hours=1)     \n",
    "                else:\n",
    "                    fechaAnterior = datos[i-1][0]\n",
    "                    fechaActual = datos[i][0]\n",
    "                    estadoActual = datos[i][2]\n",
    "                    fechaAnteriorSinMinutos = fechaAnterior - timedelta(minutes=fechaAnterior.minute) \n",
    "                    fechaActualSinMinutos = fechaActual - timedelta(minutes=fechaActual.minute) \n",
    "                    diferencia_tiempo = divmod((fechaActual-fechaAnterior).total_seconds(), 60)[0]\n",
    "                    if(not estadoActual):\n",
    "                        tiempo = 60.0\n",
    "                    if(not ((diferencia_tiempo<60.0) & (fechaAnteriorSinMinutos == fechaActualSinMinutos))):\n",
    "                        fechaActualSinMinutos -= timedelta(hours=1)\n",
    "                        while(fechaActualSinMinutos>fechaAnterior):\n",
    "                            #print([fechaActualSinMinutos,tiempo,not estadoActual])\n",
    "                            datosSalida.append([fechaActualSinMinutos,tiempo,not estadoActual,fechaCreacion])\n",
    "                            fechaActualSinMinutos -= timedelta(hours=1)\n",
    "                    #SI ES EL ÚLTIMO REGISTRO, RECORRO PARA ABAJO HASTA EL COMIENZO DEL SIUIENTE AÑO\n",
    "                    if(i==len(datos)-1):\n",
    "                        fechaActualSistema = datetime.datetime.now()\n",
    "                        fechaLimiteSuperior = datos[i][0] + (timedelta(minutes=60)-timedelta(minutes=datos[i][0].minute))\n",
    "                        estadoActual = datos[i][2]\n",
    "                        if(estadoActual):\n",
    "                            tiempo = 60.0\n",
    "                        #RECORRO PARA ABAJO\n",
    "                        if(fechaInicioAnioSiguiente>fechaActualSistema):\n",
    "                            fechaInicioAnioSiguiente = fechaActualSistema\n",
    "                        if(fechaLimiteSuperior<fechaInicioAnioSiguiente):\n",
    "                            while(fechaLimiteSuperior<fechaInicioAnioSiguiente):\n",
    "                                #print([fechaLimiteSuperior,tiempo,estadoActual])\n",
    "                                datosSalida.append([fechaLimiteSuperior,tiempo,estadoActual,fechaCreacion])\n",
    "                                fechaLimiteSuperior += timedelta(hours=1)\n",
    "\n",
    "            datos += datosSalida\n",
    "\n",
    "            return(id,datos)\n",
    "        except Exception as error:\n",
    "                ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def SepararDatos(x):\n",
    "        try:\n",
    "            id = x[0].split('-') \n",
    "            datos = list(x[1])\n",
    "            splitDatos = [(id[0],id[1],id[2],d[0],d[1],d[2],d[3]) for d in datos]\n",
    "            return splitDatos\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    def SumarizarTiempos(df_datos_tiempos):\n",
    "        try:\n",
    "            #df_datos_tiempos.filter((func.year(col('Fecha'))==2018) &\\\n",
    "            #                        (func.month(col('Fecha'))== 7)&\\\n",
    "            #                        (func.dayofmonth(col('Fecha'))==19) &\\\n",
    "            #                        (func.hour(col('Fecha'))==11)).show(1000)\n",
    "            df_datos_tiempos = df_datos_tiempos\\\n",
    "            .groupby('UNegocioId',\n",
    "                     'CentralId',\n",
    "                     'UnidadId',\n",
    "                     func.year(col('Fecha')).alias('Anio'),\n",
    "                     func.month(col('Fecha')).alias('Mes'),\n",
    "                     func.dayofmonth(col('Fecha')).alias('Dia'),\n",
    "                     func.hour(col('Fecha')).alias('Hora'),\n",
    "                     func.from_unixtime(unix_timestamp('Fecha'), 'yyyy-MM-dd HH:00:00').alias('Fecha'))\\\n",
    "            .agg(func.sum('TiempoOperacion').alias('TiempoOperacion'))\n",
    "            df_datos_tiempos.filter(col('TiempoOperacion')>60.0).show(1000)\n",
    "            return df_datos_tiempos\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREACIÓN DE VISTAS\n",
    "\n",
    "class CreacionVistas:\n",
    "    \"\"\"Otorga método rapidos para formar las vistas de datos que se necesitan para realizar cálculos\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def CrearvRepBOSNIExtendONOFF(df_evento,df_evento_dtl,df_calif,df_evt,df_mtx,df_clasif,df_categ,unidades):\n",
    "        try:\n",
    "            df_evento = df_evento.filter(df_evento.EVENTO_CLASE==4)\\\n",
    "            .withColumn('CALIF_ID',\n",
    "                        when(df_evento.TPB_CALIF_ID.isNull(),59)\\\n",
    "                        .otherwise(when(df_evento.TPB_CALIF_ID==0,59).otherwise(df_evento.TPB_CALIF_ID)))\n",
    "            \n",
    "            eventos = df_evento\\\n",
    "            .join(df_evento_dtl, df_evento.EVENTO_ID == df_evento_dtl.EVENTO_ID)\\\n",
    "            .join(df_calif, df_evento.CALIF_ID == df_calif.TPB_CALIF_ID, how='left')\\\n",
    "            .join(df_evt, df_evento.TPB_EVENTO_ID == df_evt.TPB_EVENTO_ID, how='left')\\\n",
    "            .join(df_mtx,\n",
    "                  (df_evento.TPB_EVENTO_ID == df_mtx.TPB_EVENTO_ID) &\\\n",
    "                  (df_evento.CALIF_ID == df_mtx.TPB_CALIF_ID), how='left')\\\n",
    "            .select(to_timestamp(df_evento.EVENTO_FECHA,'yyyy-MM-dd HH:mm').alias('EventoFecha'),\n",
    "                    df_evento.TPB_EVENTO_ID.alias('EvtId'),\n",
    "                    df_evt.TPB_EVENTO_CODIGO.alias('EvtCodigo'),\n",
    "                    df_evento.TPB_CALIF_ID.alias('CausalId'),\n",
    "                    df_calif.TPB_CALIF_CODIGO.alias('CausalCodigo'), \n",
    "                    df_evento.EMPRESA_ID.alias('UNegocioId'),\n",
    "                    df_evento.ESTACION_ID.alias('CentralId'),\n",
    "                    df_evento_dtl.EVENTO_DTL_ID.alias('UnidadId'),\n",
    "                    when(df_evento_dtl.EVENTO_DTL_PODER.isNull(),0)\\\n",
    "                    .otherwise(df_evento_dtl.EVENTO_DTL_PODER).alias('Potencia'),\n",
    "                    when(df_evt.TPB_EVENTO_CALCULO_ON.isNull(),False)\\\n",
    "                    .otherwise(df_evt.TPB_EVENTO_CALCULO_ON).alias('CalculoON'),\n",
    "                    when(df_evt.TPB_EVENTO_CALCULO_OFF.isNull(),False)\\\n",
    "                    .otherwise(df_evt.TPB_EVENTO_CALCULO_OFF).alias('CalculoOFF'))#,\n",
    "                    #when(df_mtx.TPB_MTX_ON_OFF.isNull(),False)\\\n",
    "                    #.otherwise(df_mtx.TPB_MTX_ON_OFF).alias('ONOFF'),\n",
    "                    #when(df_mtx.TPB_MTX_OFF_ON.isNull(),False)\\\n",
    "                    #.otherwise(df_mtx.TPB_MTX_OFF_ON).alias('OFFON')) \n",
    "            \n",
    "            ###### FILTRAMOS SOLO EVENTOS DONDE INTERVIENEN CALCULOS ON y OFF\n",
    "            eventos = eventos.filter((eventos.CalculoON==True) | (eventos.CalculoOFF==True))\\\n",
    "            .orderBy('UnidadId','EventoFecha')\n",
    "\n",
    "            eventos = eventos\\\n",
    "            .join(unidades, eventos.UnidadId == unidades.ElementoId)\\\n",
    "            .select(eventos.EventoFecha,\n",
    "                    eventos.EvtId,\n",
    "                    eventos.EvtCodigo,\n",
    "                    eventos.CausalId,\n",
    "                    eventos.CausalCodigo, \n",
    "                    eventos.UNegocioId,\n",
    "                    eventos.CentralId,\n",
    "                    eventos.UnidadId,\n",
    "                    to_timestamp(unidades.FechaCreacion,'yyyy-MM-dd HH:mm').alias('FechaCreacion'),\n",
    "                    eventos.Potencia,\n",
    "                    eventos.CalculoON,\n",
    "                    eventos.CalculoOFF)\n",
    "            \n",
    "            return eventos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "azdata_cell_guid": "db61d617-5b54-4826-b85c-4fb0c4e13676"
   },
   "outputs": [],
   "source": [
    "## Lógica de Negocio de Interrupciones del SNI\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, asc, concat, split, udf, regexp_replace,to_date,to_timestamp,round,\\\n",
    "                                  UserDefinedFunction, array, explode, struct, lit, trim, when, unix_timestamp\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import Window\n",
    "import re\n",
    "\n",
    "\n",
    "class TiempoOperacionBI:\n",
    "    \"\"\"Clase para manejar la lógica del ETL para Energía No Suministrada del SNI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self._fallasSniDA = TiempoOperacionDA()\n",
    "            self._genericDataFrame = None\n",
    "            self._df_tiempo_operacion = None\n",
    "            self._df_agt_gen = None\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def PoseeArchivos(self,rdd):\n",
    "        try:\n",
    "            num = rdd.count()\n",
    "            return num>0\n",
    "        except Exception as error:\n",
    "            return False\n",
    "    \n",
    "    def GetDataFrameHdfs(self,table_name,file_name):\n",
    "        try:\n",
    "            df = self._genericDataFrame.GetDataHdfs(table_name,file_name)\n",
    "            return df\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return None \n",
    "        \n",
    "    \n",
    "    def GetData(self,directoryNme,anio=None,schema=None):\n",
    "        \"\"\"Método para recolectar datos de un directorio específico\"\"\"\n",
    "        try:\n",
    "            sc = self._genericDataFrame.spark.sparkContext\n",
    "            \n",
    "            if anio==None:\n",
    "                ruta = self._genericDataFrame.hdfsContext.HdfsPath(directoryNme,\"file_\"+directoryNme+'*')\n",
    "                archivo = \"file_\"+directoryNme+'*'\n",
    "            else:\n",
    "                ruta = self._genericDataFrame.hdfsContext.HdfsPath(directoryNme,\"file_\"+directoryNme+'_'+str(anio))\n",
    "                archivo = \"file_\"+directoryNme+'_'+str(anio)\n",
    "                \n",
    "            rdd = sc.wholeTextFiles(ruta)\n",
    "            posee = self.PoseeArchivos(rdd)\n",
    "            \n",
    "            if posee==False:\n",
    "                return self._genericDataFrame.spark.createDataFrame([],schema)\n",
    "            \n",
    "            return self._genericDataFrame.GetDataHdfs(directoryNme,archivo)\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return None \n",
    "    \n",
    "    def ProcesarDatos(self,anio=None):\n",
    "        \"\"\"Método principal en el que se realiza la limpieza y tratamiento de lo datos\"\"\"\n",
    "        \n",
    "        try:\n",
    "            str_fecha = self._fallasSniDA.GetFechaHoraMaximaTiempoOperacion() \n",
    "            if(anio == None):\n",
    "                anio = datetime.datetime.strptime(str_fecha,'%Y-%m-%d %H:%M').year + 1\n",
    "\n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='BDTREV2'))\n",
    "            table_empresa = 'EMPRESA'\n",
    "            table_central = 'CENTRAL'\n",
    "            table_unidad = 'UNIDAD'\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_empresa)\n",
    "            df_empresa = self.GetDataFrameHdfs(table_empresa,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_central)\n",
    "            df_central = self.GetDataFrameHdfs(table_central,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_unidad)\n",
    "            df_unidad = self.GetDataFrameHdfs(table_unidad,file_name)\n",
    "            \n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='SIVO'))\n",
    "            table_cat_empresa = 'CFG_Empresa'\n",
    "            table_cat_unidad_negocio = 'CFG_UnidadNegocio'\n",
    "            table_cat_central = 'CFG_Central'\n",
    "            table_cat_unidad = 'CFG_Unidad'\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_empresa)\n",
    "            df_cat_empresa = self.GetDataFrameHdfs(table_cat_empresa,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_unidad_negocio)\n",
    "            df_cat_unidad_negocio = self.GetDataFrameHdfs(table_cat_unidad_negocio,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_central)\n",
    "            df_cat_central = self.GetDataFrameHdfs(table_cat_central,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_cat_unidad)\n",
    "            df_cat_unidad = self.GetDataFrameHdfs(table_cat_unidad,file_name)\n",
    "            \n",
    "            \n",
    "            self._genericDataFrame = GenericDataFrame(HDFSContext(DataBase='BOSNI'))\n",
    "            table_causal = 'TPB_CALIF'\n",
    "            table_evento = 'TPB_EVENTO'\n",
    "            table_clasificacion = 'TPB_CLASIFICACION_CALIF'\n",
    "            table_categoria = 'TPB_CATEGORIA_CLASIF'\n",
    "            table_mtx_calif_evento = 'MTX_CALIF_EVENTO'\n",
    "            \n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_causal)\n",
    "            df_calif = self.GetDataFrameHdfs(table_causal,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_evento)\n",
    "            df_evt = self.GetDataFrameHdfs(table_evento,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_clasificacion)\n",
    "            df_clasif = self.GetDataFrameHdfs(table_clasificacion,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_categoria)\n",
    "            df_categ = self.GetDataFrameHdfs(table_categoria,file_name)\n",
    "            \n",
    "            file_name = Utilitarios.GenerateCatalogueFileName(table_mtx_calif_evento)\n",
    "            df_mtx = self.GetDataFrameHdfs(table_mtx_calif_evento,file_name)\n",
    "\n",
    "            print('Inicio: ' + str(datetime.datetime.now()))\n",
    "\n",
    "            directoryNme = 'EVENTO'\n",
    "            df_evento = self.GetData(directoryNme,anio,EstructurasHDFS.Schema_EVENTO())\n",
    "            \n",
    "            directoryNme = 'EVENTO_DTL'\n",
    "            df_evento_dtl = self.GetData(directoryNme,anio,EstructurasHDFS.Schema_EVENTO_DTL())\n",
    "            \n",
    "            ################################### CATÁLOGO DE UNIDADES DE NEGOCIO\n",
    "            unidad_negocio = Refactorizar.DafaFrameUnidadNegocio(df_empresa,df_cat_empresa,df_cat_unidad_negocio)\n",
    "            \n",
    "            ################################### CENTRAL         \n",
    "            centrales = Refactorizar.DafaFrameEstacion(df_central,df_cat_central)\n",
    "            \n",
    "            ################################### UNIDAD\n",
    "            unidades = Refactorizar.DafaFrameElemento(df_unidad,df_cat_unidad)\n",
    "                \n",
    "            ################################### DATOS DE VISTAS\n",
    "            vBosniExtend = CreacionVistas.CrearvRepBOSNIExtendONOFF(df_evento,df_evento_dtl,df_calif,df_evt,\n",
    "                                                                    df_mtx,df_clasif,df_categ,unidades)\n",
    "\n",
    "            #vBosniExtend.filter(vBosniExtend.UnidadId==2834).show(vBosniExtend.count())\n",
    "            #vBosniExtend.filter((vBosniExtend.UnidadId==3064) &\\\n",
    "            #                    (func.year(vBosniExtend.EventoFecha)==2018) &\\\n",
    "            #                    (func.month(vBosniExtend.EventoFecha)==7) &\\\n",
    "            #                    (func.dayofmonth(vBosniExtend.EventoFecha)==19)).show(vBosniExtend.count())\n",
    "            \n",
    "            ################################### CÁLCULO DEL TIEMPO DE OPERACIÓN DE LAS UNIDADES\n",
    "            print('Proceso de generación de datos: ' + str(datetime.datetime.now()))\n",
    "            \n",
    "            #vBosniExtend = vBosniExtend.filter(vBosniExtend.UnidadId.isin({2906}))\n",
    "            \n",
    "            ################################## CALCULAMOS LOS TIEMPOS DE OPERACIÓN DE LAS HORAS DE EVENTOS\n",
    "            datosValueKey = vBosniExtend.rdd\\\n",
    "            .map(lambda x: (str(x.UNegocioId)+'-'+str(x.CentralId)+'-'+str(x.UnidadId)+'-'+\n",
    "                            str(x.EventoFecha.year)+'-'+str(x.EventoFecha.month)+'-'+str(x.EventoFecha.day)+'-'+\n",
    "                            str(x.EventoFecha.hour),\n",
    "                            [x.EventoFecha,x.CalculoON,x.FechaCreacion]))\n",
    "            #print('MAP')\n",
    "            #for d in datosValueKey.collect():\n",
    "            #    if((d[1][0].year==2018) & (d[1][0].month==7) & (d[1][0].day==19)):\n",
    "            #        print(d[0],[t for t in d[1]])\n",
    "                        \n",
    "            datosReduceValueByKey = datosValueKey.groupByKey()\n",
    "            #print('GROUPBY')\n",
    "            #for d in datosReduceValueByKey.collect():\n",
    "            #    for t in d[1]:\n",
    "            #        if((t[0].year==2018) & (t[0].month==7) & (t[0].day==19)):\n",
    "            #            print(d[0],t)\n",
    "            \n",
    "            datosConTiemposCalculados = datosReduceValueByKey.map(lambda x: Refactorizar.CalcularHorasIntermedias(x))\n",
    "\n",
    "            #print('CALCULAR HORAS')\n",
    "            #for d in datosConTiemposCalculados.collect():\n",
    "            #    for t in d[1]:\n",
    "            #        print(d[0],t)\n",
    "                        \n",
    "            #for d in datosConTiemposCalculados.collect():\n",
    "            #    for t in d[1]:\n",
    "            #        if((t[0].year==2018) & (t[0].month==7) & (t[0].day==19)):\n",
    "            #            print(d[0],t)\n",
    "            \n",
    "            rddSeparadoConTiemposCalculados = datosConTiemposCalculados.flatMap(lambda x: Refactorizar.SepararDatos(x))\n",
    "            \n",
    "            df_datos_separados = rddSeparadoConTiemposCalculados\\\n",
    "            .toDF(['UNegocioId','CentralId','UnidadId','Fecha','TiempoOperacion','CalculoOn','FechaCreacion'])\\\n",
    "            .orderBy('UnidadId','Fecha')\n",
    "            \n",
    "            #df_datos_separados.filter((func.year(col('Fecha'))==2018) &\\\n",
    "            #                          (func.month(col('Fecha'))==7) &\\\n",
    "            #                          (func.dayofmonth(col('Fecha'))==19)).show(1000)\n",
    "            ################################## RELLENAMOS LAS HORAS QUE FALTAN CON LA POTENCIA DE OPERACIÓN\n",
    "            rddValueKeyUnidad = df_datos_separados.rdd\\\n",
    "            .map(lambda x: (x[0]+'-'+x[1]+'-'+x[2],[x[3],x[4],x[5],x[6]]))\n",
    "            \n",
    "            rddGroupByUnidad = rddValueKeyUnidad.groupByKey()\n",
    "            \n",
    "            rddDatosTotales = rddGroupByUnidad.map(lambda x: Refactorizar.RellenarFechas(x))\n",
    "            \n",
    "            rddSeparadoTotales = rddDatosTotales.flatMap(lambda x: Refactorizar.SepararDatos(x))\n",
    "            \n",
    "            df_datos_tiempos = rddSeparadoTotales\\\n",
    "            .toDF(['UNegocioId','CentralId','UnidadId','Fecha','TiempoOperacion','CalculoOn','FechaCreacion'])\n",
    "            \n",
    "            #df_datos_tiempos.show()\n",
    "            ############################## SUMAR HORARIAMENTE POR CADA UNIDAD EL TIEMPO DE OPERACIÓN\n",
    "            df_datos_tiempos = Refactorizar.SumarizarTiempos(df_datos_tiempos)\n",
    "            \n",
    "            #df_datos_tiempos.orderBy('Fecha').show(df_datos_tiempos.count())\n",
    "            #df_datos_tiempos.printSchema()\n",
    "            #print(df_datos_tiempos.count())\n",
    "            ############################## PROCESAR AGENTES\n",
    "            datos_totales = self.AgregarAgentes(df_datos_tiempos,unidad_negocio,centrales,unidades)\n",
    "            \n",
    "            ################################### LIMPIEZA DE DIMENSIONES\n",
    "            ################################### LIMPIEZA DE AGENTES\n",
    "            print('Limpiar Agentes')\n",
    "            agentes = self.LimpiarAgentes(datos_totales)\n",
    "            \n",
    "            ##############################  ASIGNACIONES DE VERSIONES\n",
    "            print('Asignar Versiones')\n",
    "            datos_totales = Refactorizar.AsignarVersionesNuevas(datos_totales,self._df_agt_gen)\n",
    "            \n",
    "            ##############################  LIMPIEZ DE TIEMPO DE OPERACION\n",
    "            print('Tiempo Operación')\n",
    "            tiempo_operacion = self.LimpiarTiempoOperacion(datos_totales)\n",
    "            \n",
    "            \n",
    "            ##############################  LIMPIEZ DE FACT TIEMPO OPERACION\n",
    "            print('Limpiar fact')\n",
    "            fact_tiempo_operacion = self.LimpiarFactTiempoOperacion(datos_totales,agentes,tiempo_operacion)\n",
    "            \n",
    "            ################################### ALMACENAMIENTO DE DATOS\n",
    "            print('Almacenar Datos')\n",
    "            guardado = self.SaveData(fact_tiempo_operacion)\n",
    "            \n",
    "            print(guardado)\n",
    "            print('Fin fecha: ' + str_fecha + ' al tiempo: ' + str(datetime.datetime.now()))   \n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def AgregarAgentes(self,df_datos_tiempos,unidad_negocio,centrales,unidades):\n",
    "        try:  \n",
    "            ##### DETALLE\n",
    "            df_datos = Refactorizar.AgregarDetalles(df_datos_tiempos,unidad_negocio,centrales,unidades)\n",
    "\n",
    "            return df_datos\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def AgregarDatos(self,unidad_negocio,df_origen,df_pasos,df_total):\n",
    "        try:           \n",
    "            df_datos_totales = Refactorizar.AgregarAgtDistribucion(df_total,unidad_negocio)\n",
    "            \n",
    "            df_datos_totales = Refactorizar.AgregarAgtOrigen(df_datos_totales,unidad_negocio,df_origen)\n",
    "            \n",
    "            df_datos_totales = Refactorizar.AgregarPasos(df_datos_totales,df_pasos)\n",
    "\n",
    "            return df_datos_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    def LimpiarAgentes(self,datos):\n",
    "        try:\n",
    "            agentes_dw = Utilitarios\\\n",
    "            .ConvertPandasToSpark(self._genericDataFrame.spark,\n",
    "                                  self._fallasSniDA.GetAllAgtGen(),\n",
    "                                  Estructuras.Schema_Agt_Gen())\n",
    "            \n",
    "            udfNoVigenteUno = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR\\) ','',str(x)), StringType())\n",
    "            udfNoVigenteDos = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR \\)\\. ','',str(x)), StringType())\n",
    "            udfNoVigenteTres = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO USAR\\)\\. ','',str(x)), StringType())\n",
    "            udfNoVigenteCuatro = UserDefinedFunction(lambda x: re.sub('\\(NO VIGENTE. NO _USAR\\) ','',str(x)), StringType())\n",
    "            \n",
    "            df_agentes = datos\\\n",
    "            .select(col('EmpresaCodigo').alias('agtevt_empresa_id_bk'),\n",
    "                    func.upper(col('Empresa')).alias('agtevt_empresa'),\n",
    "                    col('UNegocioCodigo').alias('agtevt_unidad_negocio_id_bk'),\n",
    "                    func.upper(regexp_replace(regexp_replace(col('UNegocio'),'CNEL EP ',''),'CELEC EP - ','')).alias('agtevt_unidad_negocio'),\n",
    "                    when(col('EstacionCodigo').isNull(),None).otherwise(col('EstacionCodigo')).alias('agtevt_central_id_bk'),\n",
    "                    when(col('Estacion').isNull(),None).otherwise(func.upper(regexp_replace(col('Estacion'),'CENTRAL ',''))).alias('agtevt_central'),\n",
    "                    col('ElementoCodigo').alias('agtevt_unidad_id_bk'),\n",
    "                    func.upper(col('Elemento')).alias('agtevt_unidad'),\n",
    "                    col('FechaInicioOpComercial').alias('agtevt_fecha_oper_comercial'),\n",
    "                    col('Version').alias('agtevt_version'),\n",
    "                    when(col('PotEfectiva').isNull(),0).otherwise(round(col('PotEfectiva').cast(FloatType()),2)).alias('agtevt_pot_efectiva')).distinct()\n",
    "            \n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteUno(column).alias(column) for column in df_agentes.columns])\n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteDos(column).alias(column) for column in df_agentes.columns])\n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteTres(column).alias(column) for column in df_agentes.columns])  \n",
    "            df_agentes = df_agentes.select(*[udfNoVigenteCuatro(column).alias(column) for column in df_agentes.columns])\n",
    "            \n",
    "            df_agentes = Refactorizar.AsignarVersion(df_agentes,agentes_dw)\n",
    "            \n",
    "            self._df_agt_gen = df_agentes.exceptAll(agentes_dw.select(Dim_Agt_Gen.Schema()[1:]))\\\n",
    "            .withColumn('agtevt_id_pk',self._fallasSniDA.GetPkAgtGen() + \\\n",
    "                        func.row_number().over(Window.partitionBy().orderBy('agtevt_empresa_id_bk',\n",
    "                                                                            'agtevt_unidad_negocio_id_bk',\n",
    "                                                                            'agtevt_central_id_bk',\n",
    "                                                                            'agtevt_unidad_id_bk',\n",
    "                                                                            'agtevt_version')))\\\n",
    "            .select(Dim_Agt_Gen.Schema())\n",
    "            \n",
    "            df_agentes_totales = self._df_agt_gen.union(agentes_dw)\n",
    "            return df_agentes_totales            \n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "\n",
    "            \n",
    "    def LimpiarTiempoOperacion(self,datos):\n",
    "        try:\n",
    "            valueReplace = ['ENERO','FEBRERO','MARZO','ABRIL','MAYO','JUNIO','JULIO','AGOSTO','SEPTIEMBRE','OCTUBRE',\n",
    "                            'NOVIEMBRE','DICIEMBRE']        \n",
    "            paramReplace =  ['January','February','March','April','May','June','July','August','September','October',\n",
    "                             'November','December']\n",
    "            \n",
    "            df_fechas_dw = Utilitarios\\\n",
    "            .ConvertPandasToSpark(self._genericDataFrame.spark,\n",
    "                                  self._fallasSniDA.GetAllFechasTiempoOperacion(),\n",
    "                                  Estructuras.Schema_Tmp_Operacion())\n",
    "            \n",
    "            df_fechas = datos.select(to_timestamp('Fecha','yyyy-MM-dd HH:mm:ss').alias('Fecha')).distinct()\n",
    "            \n",
    "            df_fechas = df_fechas.select(func.substring(regexp_replace(regexp_replace(regexp_replace('Fecha', '-', ''),':',''),' ',''),0,10).alias('tmpop_id_pk').cast(IntegerType()),\n",
    "                                         col('Fecha').alias('tmpop_fecha'),\n",
    "                                         func.year('Fecha').alias('tmpop_anio').cast(ShortType()),\n",
    "                                         func.month('Fecha').alias('tmpop_mes_id').cast(ShortType()),\n",
    "                                         func.date_format(col('Fecha'), 'MMMMM').alias('tmpop_mes'),\n",
    "                                         func.date_format(col('Fecha'), 'dd').alias('tmpop_dia').cast(ShortType()),\n",
    "                                         func.hour('Fecha').alias('tmpop_hora').cast(ShortType()))\n",
    "            \n",
    "            df_fechas=df_fechas.na.replace(paramReplace,valueReplace,\"tmpop_mes\")\n",
    "            \n",
    "            #Conocemos si los registros estan almacenados para borrarlos del dataframe.\n",
    "            self._df_tiempo_operacion = df_fechas.exceptAll(df_fechas_dw)\n",
    "            df_fechas_totales = self._df_tiempo_operacion.union(df_fechas_dw)\n",
    "            return df_fechas_totales\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "            \n",
    "    def LimpiarFactTiempoOperacion(self,datos,agentes,tiempo_operacion):\n",
    "        try:            \n",
    "            #PK de producción\n",
    "            id_produccion = self._fallasSniDA.GetPkFactTiempoOperacion()\n",
    "            #Proceso de limpieza\n",
    "            df_fact = datos\\\n",
    "            .join(tiempo_operacion, \n",
    "                  (func.substring(regexp_replace(regexp_replace(regexp_replace(datos.Fecha, '-', ''),':',''),' ',''),0,10) == tiempo_operacion.tmpop_id_pk))\\\n",
    "            .join(agentes, \n",
    "                  (datos.EmpresaCodigo == agentes.agtevt_empresa_id_bk) & \\\n",
    "                  (datos.UNegocioCodigo == agentes.agtevt_unidad_negocio_id_bk) & \\\n",
    "                  (datos.EstacionCodigo == agentes.agtevt_central_id_bk) & \\\n",
    "                  (datos.ElementoCodigo == agentes.agtevt_unidad_id_bk) &\\\n",
    "                  (datos.Version == agentes.agtevt_version))\\\n",
    "            .select(agentes.agtevt_id_pk.alias('agtevt_id_fk'),\n",
    "                    tiempo_operacion.tmpop_id_pk.alias('tmpop_id_fk'),\n",
    "                    datos.TiempoOperacion.alias('evt_tiempo_disponible').cast(FloatType()),\n",
    "                    (60-datos.TiempoOperacion).alias('evt_tiempo_indisponible').cast(FloatType()))\\\n",
    "            .withColumn('evt_id_pk', (id_produccion + func.row_number().over(Window.partitionBy()\\\n",
    "                                                                             .orderBy('agtevt_id_fk',\n",
    "                                                                                      'tmpop_id_fk'))).cast(LongType()))\\\n",
    "            .select(Fact_Tiempo_Operacion.Schema())\n",
    "                        \n",
    "            return df_fact\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def SaveData(self,df_datos):\n",
    "        \"\"\"Método para Guardar los datos limpios una vez procesados\"\"\"\n",
    "        try:\n",
    "            save = True\n",
    "            print('Lista de Agentes Generación: ' + str(datetime.datetime.now())) \n",
    "            save = save & self._fallasSniDA.Save(self._df_agt_gen,'cen_dws.dim_agt_gen',self._genericDataFrame)\n",
    "            \n",
    "            print('Lista de Fechas de Operación: ' + str(datetime.datetime.now()))\n",
    "            save = save & self._fallasSniDA.Save(self._df_tiempo_operacion,'cen_dws.dim_tmp_operacion',self._genericDataFrame)\n",
    "\n",
    "            print('Lista de datos: ' + str(datetime.datetime.now()))\n",
    "            save = save & self._fallasSniDA.Save(df_datos,'cen_dws.fact_tiempo_operacion',self._genericDataFrame)\n",
    "\n",
    "            return save\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2020-07-20 12:47:20.082795\n",
      "Proceso de generación de datos: 2020-07-20 12:47:21.604281\n",
      "+----------+---------+--------+----+---+---+----+-----+---------------+\n",
      "|UNegocioId|CentralId|UnidadId|Anio|Mes|Dia|Hora|Fecha|TiempoOperacion|\n",
      "+----------+---------+--------+----+---+---+----+-----+---------------+\n",
      "+----------+---------+--------+----+---+---+----+-----+---------------+\n",
      "\n",
      "Limpiar Agentes\n",
      "Asignar Versiones\n",
      "Tiempo Operación\n",
      "Limpiar fact\n",
      "Almacenar Datos\n",
      "Lista de Agentes Generación: 2020-07-20 12:48:36.464801\n",
      "Lista de Fechas de Operación: 2020-07-20 12:49:52.999079\n",
      "Lista de datos: 2020-07-20 12:51:36.631081\n",
      "True\n",
      "Fin fecha: 2020-07-20 12:00:00 al tiempo: 2020-07-20 12:54:58.135936\n"
     ]
    }
   ],
   "source": [
    "proceso = TiempoOperacionBI()\n",
    "proceso.ProcesarDatos(1999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
