{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importación de clases y paquetes\n",
    "from pyspark.sql import functions as Funct\n",
    "from HDFSContext import HDFSContext\n",
    "from GenericDataFrame import GenericDataFrame\n",
    "from ExceptionManager import ExceptionManager\n",
    "from Dbscan import Dbscan\n",
    "from pyspark.sql.functions import to_timestamp, col, regexp_replace\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para limpiar los datos a nivel de mpa-reduce\n",
    "\n",
    "def Limpiar(x):\n",
    "    id = x[0]\n",
    "    datos = list(x[1])\n",
    "    salida = pd.DataFrame()\n",
    "    ids = []\n",
    "    fechas = []\n",
    "    potencia = []\n",
    "    \n",
    "    for i in range(0,len(datos)):\n",
    "        ids.append(datos[i][0])\n",
    "        fechas.append(datos[i][1])\n",
    "        potencia.append(datos[i][2])\n",
    "    \n",
    "    salida['Id'] = ids\n",
    "    salida['Fecha'] = fechas\n",
    "    salida['Potencia'] = potencia\n",
    "    \n",
    "    otliers,datos_limpios = Limpiar_Datos(salida.set_index('Fecha'))\n",
    "    \n",
    "    return (id, [otliers,datos_limpios,salida])\n",
    "\n",
    "def Limpiar_Datos(datosDestino):\n",
    "    dbscan = Dbscan(datosDestino)\n",
    "    dbscan.Limpiar_outliers()\n",
    "    dbscan.Resumen_Datos()\n",
    "    return dbscan.outliers,dbscan.datos_limpios\n",
    "\n",
    "def Completar_Datos(x,tipo,fecha_inicio,fecha_fin,extract_data_tranformador):\n",
    "    id = x[0]\n",
    "    datos = list(x[1])\n",
    "    \n",
    "    outliers = datos[0].reset_index()\n",
    "    datos_limpios = datos[1].reset_index()\n",
    "    datos_originales = datos[2]\n",
    "    \n",
    "    \n",
    "    if(datos_limpios['Id'].count()==0):\n",
    "        datos_limpios = outliers\n",
    "    \n",
    "    resultado = pd.merge(datos_originales,datos_limpios,on='Id')\n",
    "    resultado = resultado[resultado['Calidad'].isin(['Normal','AL','AL.L5','AL.L6','L1'])]\n",
    "    \n",
    "    if(resultado['Id'].count()==0):\n",
    "        resultado = pd.merge(datos_originales,datos_limpios,on='Id')\n",
    "    \n",
    "    \n",
    "    puntoInicial = np.array([datetime.datetime(fecha_inicio.year,fecha_inicio.month,fecha_inicio.day,0,0,0) - timedelta(seconds=1)])\n",
    "    puntoInicial = pd.DataFrame(puntoInicial,columns=['Fecha'])\n",
    "    \n",
    "    puntoFinal = np.array([datetime.datetime(fecha_inicio.year,fecha_inicio.month,fecha_inicio.day,23,59,59) + timedelta(seconds=1)])\n",
    "    puntoFinal = pd.DataFrame(puntoFinal,columns=['Fecha'])\n",
    "\n",
    "    resultado = resultado[['Id','Fecha_x','Potencia_x','Calidad']]\\\n",
    "    .rename(columns={'Fecha_x':'Fecha','Potencia_x':'Potencia'})\n",
    "    \n",
    "    maximo = resultado['Id'].values[-1]\n",
    "    \n",
    "    resultado = pd.merge(puntoInicial,resultado,on='Fecha',how='outer')\n",
    "    resultado = pd.merge(puntoFinal,resultado,on='Fecha',how='outer')\n",
    "\n",
    "    values = {'Fecha':datetime.datetime(1970,1,1,0,0,0),'Id':maximo,'Potencia':0,'Calidad':'Normal'}\n",
    "    resultado = resultado.fillna(value=values)\n",
    "    resultado = resultado.set_index('Fecha').resample('S').mean()\n",
    "    resultado = resultado[1:-1]\n",
    "    resultado = resultado.interpolate(method='values')\n",
    "    resultado = resultado.interpolate(method='values',limit_direction='backward')\n",
    "    \n",
    "    resultado_segundo = resultado.copy()\n",
    "    \n",
    "    resultado = resultado.resample('T').max()\n",
    "    resultado['Elemento'] = id\n",
    "    resultado['Tipo'] = tipo\n",
    "\n",
    "   # if(tipo=='Origen'):\n",
    "    #    calidad_circuitos = Obtener_Calidad_Circuitos(fecha_inicio,fecha_fin,circuito=id,destino=False)\n",
    "    #else:\n",
    "    #    calidad_circuitos = Obtener_Calidad_Circuitos(fecha_inicio,fecha_fin,circuito=id,destino=True)\n",
    "        \n",
    "    #calidad_circuitos = calidad_circuitos.toPandas().set_index('Fecha')[['Calidad','LimMaxOperacion',\n",
    "    #                                                                     'LimOperacionContinuo','LimTermico',\n",
    "    #                                                                     'TagCalidad','TagPotencia']]\n",
    "    \n",
    "    calidad_trafo = extract_data_tranformador.where(extract_data_tranformador.CodigoTrafo == id)\n",
    "    \n",
    "    calidad_circuitos = calidad_circuitos.toPandas().set_index('Fecha')[['Calidad','MVA_Emergente',\n",
    "                                                                         'MVA_NominalFOA','Subestacion',\n",
    "                                                                         'UnidadNegocio','CodigoTrafo']]\n",
    "    \n",
    "    \n",
    "    resultado = pd.merge(calidad_trafo,resultado,on='Fecha')\n",
    "\n",
    "    resultado_segundo = resultado_segundo.reset_index().groupby('Potencia')['Fecha'].min().reset_index()\\\n",
    "    .set_index('Potencia').rename(columns={'Fecha':'FechaMaximo'})\n",
    "    resultado = resultado.reset_index().set_index('Potencia')\n",
    "    resultado = pd.merge(resultado_segundo,resultado,on='Potencia')\n",
    "    resultado = resultado.reset_index().set_index('Fecha').sort_index().reset_index()\n",
    "    \n",
    "    return (id,resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElementosTransmisionBL():\n",
    "    \"\"\"Clase que procesa la lógica del negocio y limpieza de datos de los elementos de Transmisión.\"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self._genericDataFrame=GenericDataFrame(HDFSContext(Path='PI_INTEGRATOR',DataBase='TRANSMISION',Schema='')) \n",
    "        \n",
    "    def Extract_data_Transformador(self, fecha_inicio,fecha_fin,trafo):\n",
    "        \"\"\"Método que realiza la extracción de datos de transformadores del HDFS\"\"\"\n",
    "        \n",
    "        try: \n",
    "            df_transformadores = self._genericDataFrame.GetDataCsvHdfs(tableName='',fileName='transformadores_*',header='true',\n",
    "                                                                       delimiter='\\t')\n",
    "            \n",
    "            df_transformadores = df_transformadores.select('Id', \n",
    "                                               to_timestamp(col('Estampa de Tiempo'),'dd/MM/yy HH:mm:ss').alias('Fecha'),\n",
    "                                               col('Calidad Potencia').alias('Calidad'),\n",
    "                                               regexp_replace(col('MVA_Emergente'),',','.').cast('double').alias('MVA_Emergente'),\n",
    "                                               regexp_replace(col('MVA_NominalFOA'),',','.').cast('double').alias('MVA_NominalFOA'),\n",
    "                                               'Subestacion','UnidadNegocio',\n",
    "                                               col('Codigo').alias('CodigoTrafo'),\n",
    "                                               col('Nombre').alias('NombreTrafo'), \n",
    "                                               regexp_replace(col('Potencia Aparente'),',','.').cast('double').alias('Potencia'))\n",
    "            \n",
    "            if trafo is None: \n",
    "                trafos = df_transformadores.select('*').where(df_transformadores.Fecha.between(fecha_inicio, fecha_fin))\n",
    "            else: \n",
    "                trafos = df_transformadores.select('*').where((df_transformadores.CodigoTrafo == trafo)\n",
    "                                                            & (df_transformadores.Fecha.between(fecha_inicio, fecha_fin)))\n",
    "                \n",
    "            return trafos\n",
    "    \n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "            \n",
    "    def Transform_data_Transformador(self, fecha_inicio,fecha_fin,extract_data_tranformador): \n",
    "        \"\"\"Método que realiza la transformación de datos de los transformadores. Se aplica el algoritmo DBCSAN para limpiar y \n",
    "        eliminar datos atipicos. \"\"\"\n",
    "        \n",
    "        try: \n",
    "            \n",
    "            #Procesamiento de cada transformador \n",
    "            \n",
    "            transformador_map = extract_data_tranformador.rdd.map(lambda x: (x.CodigoTrafo,[x.Id,x.Fecha,x.Potencia, x.Calidad]))\n",
    "            transformador_map_groupBy = transformador_map.groupByKey()\n",
    "            transformador_map_clear = transformador_map_groupBy.map(lambda x: Limpiar(x))\n",
    "            transformador_map_complete = transformador_map_clear.map(lambda x: Completar_Datos(x,fecha_inicio,fecha_fin,extract_data_tranformador))\n",
    "            \n",
    "            print(transformador_map_complete.collect())\n",
    "\n",
    "        except Exception as error:\n",
    "            ExceptionManager.Treatment(error)\n",
    "    \n",
    "    def Elt_main(self, fecha_inicio, fecha_fin,trafo=None):\n",
    "        \"\"\"Método que procesa todo el flujo de proceso del ETL.\"\"\"\n",
    "        print(\"---- Proceso de ETL del Análisis de potencia de Transformadores ---- \\n\")\n",
    "        print(\"DATAMART: dwh_sirio\")\n",
    "        print(\"TABLE DE HECHOS: fact_transformador \\n\")\n",
    "        \n",
    "        print(\"1. Extracción de datos\")\n",
    "        extract_data_tranformador = self.Extract_data_Transformador(fecha_inicio, fecha_fin,trafo)\n",
    "        \n",
    "        if(extract_data_tranformador.count()==0):\n",
    "            print(' **** WARNING: No existe Datos de Transformadores para Procesar. '+str(datetime.datetime.now()))\n",
    "            return \n",
    "            \n",
    "        print(\"2. Transformación de datos\")\n",
    "        transform_data = self.Transform_data_Transformador(fecha_inicio,fecha_fin, extract_data_tranformador)\n",
    "        \n",
    "        #print(\"3. Cargar  datos\\n\")\n",
    "        #self.Load_data(transform_data,\"cen_dws.fact_tmp_descon_carga\")\n",
    "        #self.Load_data(transform_data_horas,\"cen_dws.dim_hora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Proceso de ETL del Análisis de potencia de Transformadores ---- \n",
      "\n",
      "DATAMART: dwh_sirio\n",
      "TABLE DE HECHOS: fact_transformador \n",
      "\n",
      "1. Extracción de datos\n",
      "2. Transformación de datos\n",
      "Could not serialize object: Py4JError: An error occurred while calling o210.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 590, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 786, in save_tuple\n",
      "    save(element)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 816, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 840, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 816, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 400, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 816, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 840, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 524, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o210.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ob = ElementosTransmisionBL()\n",
    "fecha_inicio = datetime.datetime.strptime('2019-10-10 00:00', '%Y-%m-%d %H:%M')\n",
    "fecha_fin = datetime.datetime.strptime('2019-10-10 23:59', '%Y-%m-%d %H:%M')\n",
    "\n",
    "ob.Elt_main(fecha_inicio, fecha_fin,'AMB138069T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
