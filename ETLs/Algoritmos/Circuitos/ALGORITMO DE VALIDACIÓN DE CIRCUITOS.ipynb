{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorítmo de limpieza de potencia aparente de Circuitos\n",
    "\n",
    "El presente algorítmo permitirá obtener valores de potencia aparente validada para almacenarla en el DWH, la estampa de tiempo de la validación es de 1 minuto.\n",
    "\n",
    "### Pasos a seguir durante el proceso:\n",
    "\n",
    " - Limpieza de outliers con el Método: DBSCAN\n",
    " - Aplicación del algorÍtmo de validación de máximos y mínimos\n",
    " - Almacenamiento de los datos validados en el DWH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de outliers con el Método: DBSCAN\n",
    "\n",
    "Utilizaremos la biblioreca **sklearn** la cual tiene listo el algoritmo DBSCAN para ser implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importación de clases y paquetes\n",
    "\n",
    "from ExceptionManager import ExceptionManager\n",
    "from HDFSContext import HDFSContext\n",
    "from GenericDataFrame import GenericDataFrame\n",
    "from DBContextDw import DBContextDw\n",
    "from EtlDimensionAL import EtlDimensionAL\n",
    "from Queries import Queries\n",
    "from GetCircuitos import GetCircuitos\n",
    "from Dbscan import Dbscan\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,TimestampType,FloatType,StringType,IntegerType\n",
    "from pyspark.sql.functions import when,date_format\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import to_timestamp, col, regexp_replace\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtención de los datos de Circuitos desde el HDFS ( POTENCIA DESTINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "\n",
    "def Obtener_Circuitos(fecha_inicio,fecha_fin,circuito=None,destino=False):\n",
    "    # Datos De circuitos (DataFrame Pyspark)\n",
    "    getCircuitos = GetCircuitos(TableName = '')\n",
    "    if(destino):\n",
    "        circuitos = getCircuitos.GetDataDestino(fecha_inicio,fecha_fin,fileName='circuitos_destino_*',circuito=circuito)\n",
    "    else:\n",
    "        circuitos = getCircuitos.GetDataOrigen(fecha_inicio,fecha_fin,fileName='circuitos_origen_*',circuito=circuito)\n",
    "    return circuitos\n",
    "\n",
    "def Obtener_Calidad_Circuitos(fecha_inicio,fecha_fin,circuito=None,destino=False):\n",
    "    # Datos De circuitos (DataFrame Pyspark)\n",
    "    getCircuitos = GetCircuitos(TableName = '')\n",
    "    if(destino):\n",
    "        circuitos = getCircuitos.GetDataQualityDestino(fecha_inicio,fecha_fin,fileName='calidad_circuitos_destino_*',circuito=circuito)\n",
    "    else:\n",
    "        circuitos = getCircuitos.GetDataQualityOrigen(fecha_inicio,fecha_fin,fileName='calidad_circuitos_origen_*',circuito=circuito)\n",
    "    return circuitos\n",
    "    \n",
    "def Transformar_Pandas(df_pyspark,column_index):\n",
    "    # Datos a pandas dataframe\n",
    "    datosDestino = circuitosDestino.toPandas().set_index(column_index)\n",
    "    return datosDestino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de Limpieza\n",
    "\n",
    "def Limpiar_Datos(datosDestino):\n",
    "    dbscan = Dbscan(datosDestino)\n",
    "    dbscan.Limpiar_outliers()\n",
    "    dbscan.Resumen_Datos()\n",
    "    return dbscan.outliers,dbscan.datos_limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenar datos pre-procesados\n",
    "\n",
    "def Almacenar_Datos_Sin_Outliers(df_pyspark,circuito):\n",
    "    path = genericDataFrame.hdfsContext.HdfsPath('',circuito+'.json')\n",
    "    print(path)\n",
    "    df_pyspark.fillna('None').write.format('json').mode('overwrite').save(path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para limpiar los datos a nivel de mpa-reduce\n",
    "\n",
    "def Limpiar(x):\n",
    "    id = x[0]\n",
    "    datos = list(x[1])\n",
    "    salida = pd.DataFrame()\n",
    "    ids = []\n",
    "    fechas = []\n",
    "    potencia = []\n",
    "    calidad = []\n",
    "    for i in range(0,len(datos)):\n",
    "        ids.append(datos[i][0])\n",
    "        fechas.append(datos[i][1])\n",
    "        potencia.append(datos[i][2])\n",
    "        calidad.append(datos[i][3])\n",
    "    \n",
    "    salida['Id'] = ids\n",
    "    salida['Fecha'] = fechas\n",
    "    salida['Potencia'] = potencia\n",
    "    salida['Calidad'] = calidad\n",
    "    \n",
    "    outliers,datos_limpios = Limpiar_Datos(salida.set_index('Fecha'))\n",
    "    \n",
    "    return (id, [outliers,datos_limpios,salida])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para completar los datos por Interpolación\n",
    "\n",
    "def Completar_Datos(x,tipo,fecha_inicio,fecha_fin):\n",
    "    id = x[0]\n",
    "    datos = list(x[1])\n",
    "    \n",
    "    outliers = datos[0].reset_index()\n",
    "    datos_limpios = datos[1].reset_index()\n",
    "    datos_originales = datos[2]\n",
    "    \n",
    "    \n",
    "    if(datos_limpios['Id'].count()==0):\n",
    "        datos_limpios = outliers\n",
    "    \n",
    "    resultado = pd.merge(datos_originales,datos_limpios,on='Id')\n",
    "    resultado = resultado[resultado['Calidad'].isin(['Normal','AL','AL.L5','AL.L6','L1'])]\n",
    "    \n",
    "    if(resultado['Id'].count()==0):\n",
    "        resultado = pd.merge(datos_originales,datos_limpios,on='Id')\n",
    "    \n",
    "    \n",
    "    puntoInicial = np.array([datetime.datetime(fecha_inicio.year,fecha_inicio.month,fecha_inicio.day,0,0,0) - timedelta(seconds=1)])\n",
    "    puntoInicial = pd.DataFrame(puntoInicial,columns=['Fecha'])\n",
    "    \n",
    "    puntoFinal = np.array([datetime.datetime(fecha_inicio.year,fecha_inicio.month,fecha_inicio.day,23,59,59) + timedelta(seconds=1)])\n",
    "    puntoFinal = pd.DataFrame(puntoFinal,columns=['Fecha'])\n",
    "\n",
    "    resultado = resultado[['Id','Fecha_x','Potencia_x','Calidad']]\\\n",
    "    .rename(columns={'Fecha_x':'Fecha','Potencia_x':'Potencia'})\n",
    "    \n",
    "    maximo = resultado['Id'].values[-1]\n",
    "    \n",
    "    resultado = pd.merge(puntoInicial,resultado,on='Fecha',how='outer')\n",
    "    resultado = pd.merge(puntoFinal,resultado,on='Fecha',how='outer')\n",
    "\n",
    "    values = {'Fecha':datetime.datetime(1970,1,1,0,0,0),'Id':maximo,'Potencia':0,'Calidad':'Normal'}\n",
    "    resultado = resultado.fillna(value=values)\n",
    "    resultado = resultado.set_index('Fecha').resample('S').mean()\n",
    "    resultado = resultado[1:-1]\n",
    "    resultado = resultado.interpolate(method='values')\n",
    "    resultado = resultado.interpolate(method='values',limit_direction='backward')\n",
    "    \n",
    "    resultado_segundo = resultado.copy()\n",
    "    \n",
    "    resultado = resultado.resample('T').max()\n",
    "    resultado['Elemento'] = id\n",
    "    resultado['Tipo'] = tipo\n",
    "\n",
    "    if(tipo=='Origen'):\n",
    "        calidad_circuitos = Obtener_Calidad_Circuitos(fecha_inicio,fecha_fin,circuito=id,destino=False)\n",
    "    else:\n",
    "        calidad_circuitos = Obtener_Calidad_Circuitos(fecha_inicio,fecha_fin,circuito=id,destino=True)\n",
    "        \n",
    "    calidad_circuitos = calidad_circuitos.toPandas().set_index('Fecha')[['Calidad','LimMaxOperacion',\n",
    "                                                                         'LimOperacionContinuo','LimTermico',\n",
    "                                                                         'TagCalidad','TagPotencia']]\n",
    "    \n",
    "    resultado = pd.merge(calidad_circuitos,resultado,on='Fecha')\n",
    "\n",
    "    resultado_segundo = resultado_segundo.reset_index().groupby('Potencia')['Fecha'].min().reset_index()\\\n",
    "    .set_index('Potencia').rename(columns={'Fecha':'FechaMaximo'})\n",
    "    resultado = resultado.reset_index().set_index('Potencia')\n",
    "    resultado = pd.merge(resultado_segundo,resultado,on='Potencia')\n",
    "    resultado = resultado.reset_index().set_index('Fecha').sort_index().reset_index()\n",
    "    \n",
    "    return (id,resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algoritmo de validación de datos\n",
    "\n",
    "def Algoritmo_Validacion_Datos(df,deltaDiffCircuito):         \n",
    "    df_origen = df.filter(col('Tipo')=='Origen')\\\n",
    "    .select(col('Fecha').alias('FechaOrigen'),\n",
    "            col('Potencia').alias('PotenciaOrigen'),\n",
    "            col('Calidad').alias('CalidadOrigen'),\n",
    "            col('LimMaxOperacion').alias('LimMaxOperacionOrigen'),\n",
    "            col('LimOperacionContinuo').alias('LimOperacionContinuoOrigen'),\n",
    "            col('LimTermico').alias('LimTermicoOrigen'),\n",
    "            col('TagCalidad').alias('TagCalidadOrigen'),\n",
    "            col('TagPotencia').alias('TagPotenciaOrigen'),\n",
    "            col('Elemento').alias('ElementoOrigen'),\n",
    "            col('FechaMaximo').alias('FechaMaximoOrigen'))\\\n",
    "    .withColumn('SuperiorOrigen',func.round(col('PotenciaOrigen')/col('LimTermicoOrigen'),5))\\\n",
    "    .withColumn('CargabilidadOrigen',func.round(col('PotenciaOrigen')/col('LimOperacionContinuoOrigen'),5))\n",
    "    \n",
    "    df_destino = df.filter(col('Tipo')=='Destino')\\\n",
    "    .select(col('Fecha').alias('FechaDestino'),\n",
    "            col('Potencia').alias('PotenciaDestino'),\n",
    "            col('Calidad').alias('CalidadDestino'),\n",
    "            col('LimMaxOperacion').alias('LimMaxOperacionDestino'),\n",
    "            col('LimOperacionContinuo').alias('LimOperacionContinuoDestino'),\n",
    "            col('LimTermico').alias('LimTermicoDestino'),\n",
    "            col('TagCalidad').alias('TagCalidadDestino'),\n",
    "            col('TagPotencia').alias('TagPotenciaDestino'),\n",
    "            col('Elemento').alias('ElementoDestino'),\n",
    "            col('FechaMaximo').alias('FechaMaximoDestino'))\\\n",
    "    .withColumn('SuperiorDestino',func.round(col('PotenciaDestino')/col('LimTermicoDestino'),5).cast('double'))\\\n",
    "    .withColumn('CargabilidadDestino',func.round(col('PotenciaDestino')/col('LimOperacionContinuoDestino'),5).cast('double'))\n",
    "    \n",
    "    df = df_origen.join(df_destino,\n",
    "                        (df_origen.FechaOrigen==df_destino.FechaDestino) &\\\n",
    "                        (df_origen.ElementoOrigen==df_destino.ElementoDestino))\\\n",
    "    .select(when(col('FechaOrigen').isNull(),col('FechaDestino')).otherwise(col('FechaOrigen')).alias('Fecha'),\n",
    "            when(col('ElementoOrigen').isNull(),col('ElementoDestino')).otherwise(col('ElementoOrigen')).alias('Elemento'),\n",
    "            when(col('LimOperacionContinuoOrigen').isNull(),col('LimOperacionContinuoDestino')).otherwise(col('LimOperacionContinuoOrigen')).alias('LimOperacionContinuo'),\n",
    "            when(col('LimTermicoOrigen').isNull(),col('LimTermicoDestino')).otherwise(col('LimTermicoOrigen')).alias('LimTermico'),\n",
    "            when(col('LimMaxOperacionOrigen').isNull(),col('LimMaxOperacionDestino')).otherwise(col('LimMaxOperacionOrigen')).alias('LimMaxOperacion'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('TagPotenciaOrigen'))\\\n",
    "                 .otherwise(col('TagPotenciaDestino')))\\\n",
    "            .otherwise(col('TagPotenciaDestino')).alias('TagPotencia'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('PotenciaOrigen'))\\\n",
    "                 .otherwise(col('PotenciaDestino')))\\\n",
    "            .otherwise(col('PotenciaDestino')).alias('Potencia'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('SuperiorOrigen'))\\\n",
    "                 .otherwise(col('SuperiorDestino')))\\\n",
    "            .otherwise(col('SuperiorDestino')).alias('Superior'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('CargabilidadOrigen'))\\\n",
    "                 .otherwise(col('CargabilidadDestino')))\\\n",
    "            .otherwise(col('CargabilidadDestino')).alias('Cargabilidad'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('FechaMaximoOrigen'))\\\n",
    "                 .otherwise(col('FechaMaximoDestino')))\\\n",
    "            .otherwise(col('FechaMaximoDestino')).alias('FechaMaximo'),\n",
    "            \n",
    "            when(col('PotenciaOrigen')>=col('PotenciaDestino'),\n",
    "                 when((col('PotenciaOrigen')-col('PotenciaDestino'))<deltaDiffCircuito,col('CalidadOrigen'))\\\n",
    "                 .otherwise(col('CalidadDestino')))\\\n",
    "            .otherwise(col('CalidadDestino')).alias('Calidad')).orderBy('Elemento','Fecha')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamiento de los datos procesados\n",
    "\n",
    "def Load_Upsert_data(transform_data,fecha_inicio,fecha_fin,accesoDatos):\n",
    "    \"\"\"Método que realiza la lógica de creación de queries para el método upsert. \"\"\"\n",
    "    result_transact = []\n",
    "    \n",
    "    # Eliminación de los datos antes de la carga respectiva\n",
    "    result = accesoDatos.\n",
    "    \n",
    "    #\n",
    "    transform_data_map = transform_data.rdd.map(lambda x: (x.agt_id_pk, [x.agt_empresa_id_bk, x.agt_empresa, \n",
    "                                                                         x.agt_region_id_bk, x.agt_region,\n",
    "                                                                         x.agt_und_negocio_id_bk, x.agt_und_negocio,\n",
    "                                                                         x.agt_clase_unegocio_id_bk, x.agt_clase_unegocio,\n",
    "                                                                         x.agt_estacion_id_bk, x.agt_estacion,\n",
    "                                                                         x.agt_tipo_estacion_id_bk, x.agt_tipo_estacion,\n",
    "                                                                         x.agt_grupo_gen_id_bk, x.agt_grupo_gen,\n",
    "                                                                         x.agt_voltaje_id_bk, x.agt_voltaje,\n",
    "                                                                         x.agt_tipo_elemento_id_bk, x.agt_tipo_elemento,\n",
    "                                                                         x.agt_elemento_id_bk, x.agt_elemento,\n",
    "                                                                         x.agt_operacion_comercial,x.fecha_carga]))\n",
    "\n",
    "    querys_data_insert = transform_data_map.map(lambda x: Queries.Upsert_Query_Dim_Agente(x))\n",
    "    #for index in querys_data_insert.collect():\n",
    "    #    print(index[0],index[1])\n",
    "    for index in querys_data_insert.collect():\n",
    "        res = []\n",
    "        pk = index[0]\n",
    "        query = index[1]\n",
    "\n",
    "        result = self._accesoDatos.UpsertDimension(query)\n",
    "\n",
    "        res.extend([pk,result])\n",
    "        result_transact.append(res)            \n",
    "\n",
    "    schema = StructType([\n",
    "            StructField(\"pk\", IntegerType(),False),\n",
    "            StructField(\"Result\", BooleanType(),False)\n",
    "        ])\n",
    "\n",
    "    result_transact =  self._genericDataFrame.spark.createDataFrame(result_transact,schema=schema)\n",
    "\n",
    "    return result_transact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de Consolidación de datos.\n",
    "\n",
    "def Integridad_Datos(df,agentes,tiempo,hora,fecha_inicio,fecha_fin,accesoDatos):\n",
    "    agentes = agentes.filter((col('agt_clase_unegocio_id_bk')=='TRA') & (col('agt_tipo_elemento_id_bk')==3))\n",
    "    \n",
    "    df = df\\\n",
    "    .join(tiempo, date_format(df.Fecha,'yyyyMMdd').cast('int')==tiempo.tmpo_id_pk)\\\n",
    "    .join(hora, date_format(df.Fecha,'HHmm').cast('smallint')==hora.hora_id_pk)\\\n",
    "    .join(agentes, df.Elemento == agentes.agt_elemento_id_bk,how='left')\\\n",
    "    .select(date_format(df.Fecha,'yyyyMMdd').cast('int').alias('crk_tmpo_id_fk'),\n",
    "            date_format(df.Fecha,'HHmm').cast('smallint').alias('crk_hora_id_fk'),\n",
    "            agentes.agt_id_pk.alias('crk_agt_id_fk'),\n",
    "            df.Potencia.alias('crk_potencia_aparente'),\n",
    "            df.Superior.alias('crk_superior'),\n",
    "            df.Cargabilidad.alias('crk_cargabilidad'),\n",
    "            df.LimTermico.alias('crk_limite_termico'),\n",
    "            df.LimMaxOperacion.alias('crk_limite_max_operacion'),\n",
    "            df.LimOperacionContinuo.alias('crk_limite_operacion_cont'),\n",
    "            df.FechaMaximo.alias('crk_fecha_maximo'),\n",
    "            df.TagPotencia.alias('crk_tag_potencia'),\n",
    "            df.Calidad.alias('crk_calidad'))\n",
    "    \n",
    "    faltantes = df.filter(agentes.agt_id_pk.isNull())\n",
    "    \n",
    "    #if(~faltantes.rdd.isEmpty()):\n",
    "    #    return None\n",
    "    \n",
    "    Load_Upsert_data(df,fecha_inicio,fecha_fin,accesoDatos)\n",
    "    #df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución Automática\n",
    "\n",
    "class LimpiezaBL():\n",
    "    \"\"\"Lógica de negocio para realizar la limpieza de datos de circuitos.\"\"\"\n",
    "    def __init__ (self):\n",
    "        dbContext = DBContextDw(Database='dwh_sirio',urlDriver='/home/jovyan/work/postgresql-42.2.12.jar')\n",
    "        self._accesoDatos = EtlDimensionAL(dbContext)\n",
    "        self._genericDataFrame = GenericDataFrame(HDFSContext(Path='PI_INTEGRATOR',DataBase='TRANSMISION',Schema=''))\n",
    "        self._sc = self._genericDataFrame.spark.sparkContext\n",
    "        self._deltaDiffCircuito = 110\n",
    "        \n",
    "    def Ejecucion(self,fecha_inicio, fecha_fin):\n",
    "        if(fecha_inicio is None or fecha_fin is None):\n",
    "            fecha_inicio = datetime.datetime.combine(datetime.date.today(),datetime.datetime.min.time()) \n",
    "            fecha_fin = fecha_inicio + timedelta(seconds=86399)\n",
    "\n",
    "        circuitos_origen = Obtener_Circuitos(fecha_inicio,fecha_fin,destino=False)\n",
    "        circuitos_destino = Obtener_Circuitos(fecha_inicio,fecha_fin,destino=True)\n",
    "\n",
    "        if((circuitos_origen.count()==0) or (circuitos_destino.count()==0)):\n",
    "            print('No existe Datos a Procesar. '+str(datetime.datetime.now()))\n",
    "            return False\n",
    "\n",
    "        #Procesamos por cada circuito para limpiar - Datos Origen\n",
    "        circuitos_map_origen = circuitos_origen.rdd.map(lambda x: (x.Circuito,[x.Id,x.Fecha,x.Potencia,x.Calidad]))\n",
    "        circuitos_map_group_origen = circuitos_map_origen.groupByKey()\n",
    "        circuitos_pandas_origen = circuitos_map_group_origen.map(lambda x: Limpiar(x))\n",
    "        circuitos_pandas_origen = circuitos_pandas_origen.map(lambda x: Completar_Datos(x,'Origen',fecha_inicio,fecha_fin))\n",
    "\n",
    "        #Procesamos por cada circuito para limpiar - Datos Destino\n",
    "        circuitos_map_destino = circuitos_destino.rdd.map(lambda x: (x.Circuito,[x.Id,x.Fecha,x.Potencia,x.Calidad]))\n",
    "        circuitos_map_group_destino = circuitos_map_destino.groupByKey()\n",
    "        circuitos_pandas_destino = circuitos_map_group_destino.map(lambda x: Limpiar(x))\n",
    "        circuitos_pandas_destino = circuitos_pandas_destino.map(lambda x: Completar_Datos(x,'Destino',fecha_inicio,fecha_fin))\n",
    "\n",
    "        #Unimos ambos RDDs\n",
    "        datos_completos = self._sc.union([circuitos_pandas_origen,circuitos_pandas_destino])\n",
    "        datos_completos = datos_completos.map(lambda x: (x[1].to_numpy()))\n",
    "        datos_completos = datos_completos.flatMap(lambda x: [(datetime.datetime.strptime(datetime.datetime.strftime(d[0],'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S'),\n",
    "                                                              d[1],\n",
    "                                                              datetime.datetime.strptime(datetime.datetime.strftime(d[2],'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S'),\n",
    "                                                              d[3],d[4],d[5],d[6],d[7],d[8],d[9],\n",
    "                                                              d[10]) for d in list(x)])\n",
    "\n",
    "        schema = StructType([StructField('Fecha', TimestampType(), False),\n",
    "                             StructField('Potencia', FloatType(), False),\n",
    "                             StructField('FechaMaximo', TimestampType(), False),\n",
    "                             StructField('Calidad', StringType(), False),\n",
    "                             StructField('LimMaxOperacion', FloatType(), False),\n",
    "                             StructField('LimOperacionContinuo', FloatType(), False),\n",
    "                             StructField('LimTermico', FloatType(), False),\n",
    "                             StructField('TagCalidad', StringType(), False),\n",
    "                             StructField('TagPotencia', StringType(), False),\n",
    "                             StructField('Elemento', StringType(), False),\n",
    "                             StructField('Tipo', StringType(), False)])\n",
    "\n",
    "        df = self._genericDataFrame.spark.createDataFrame(datos_completos,schema)\n",
    "        df = Algoritmo_Validacion_Datos(df,self._deltaDiffCircuito)\n",
    "        df_elementos = self._accesoDatos.GetAllData('cen_dws.dim_agente')\n",
    "        df_tiempo = self._accesoDatos.GetAllData('cen_dws.dim_tiempo')\n",
    "        df_horas = self._accesoDatos.GetAllData('cen_dws.dim_hora')\n",
    "        \n",
    "        Integridad_Datos(df,df_elementos,df_tiempo,df_horas,fecha_inicio,fecha_fin,self._accesoDatos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+---------------------+------------+----------------+------------------+------------------------+-------------------------+-------------------+--------------------+-----------+\n",
      "|crk_tmpo_id_fk|crk_hora_id_fk|crk_agt_id_fk|crk_potencia_aparente|crk_superior|crk_cargabilidad|crk_limite_termico|crk_limite_max_operacion|crk_limite_operacion_cont|   crk_fecha_maximo|    crk_tag_potencia|crk_calidad|\n",
      "+--------------+--------------+-------------+---------------------+------------+----------------+------------------+------------------------+-------------------------+-------------------+--------------------+-----------+\n",
      "|      20191010|           148|           29|             50.95693|     0.61766|         0.30883|              82.5|                    93.0|                    165.0|2019-10-10 01:48:15|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|           148|         1942|             50.95693|     0.61766|         0.30883|              82.5|                    93.0|                    165.0|2019-10-10 01:48:15|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|           833|           29|             47.64805|     0.57755|         0.28878|              82.5|                    93.0|                    165.0|2019-10-10 08:33:25|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|           833|         1942|             47.64805|     0.57755|         0.28878|              82.5|                    93.0|                    165.0|2019-10-10 08:33:25|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1238|           29|            47.678932|     0.57793|         0.28896|              82.5|                    93.0|                    165.0|2019-10-10 12:38:57|BANOS138CAGOY_2_S...|     Normal|\n",
      "|      20191010|          1238|         1942|            47.678932|     0.57793|         0.28896|              82.5|                    93.0|                    165.0|2019-10-10 12:38:57|BANOS138CAGOY_2_S...|     Normal|\n",
      "|      20191010|          1342|           29|            47.814922|     0.57957|         0.28979|              82.5|                    93.0|                    165.0|2019-10-10 13:42:26|BANOS138CAGOY_2_S...|     Normal|\n",
      "|      20191010|          1342|         1942|            47.814922|     0.57957|         0.28979|              82.5|                    93.0|                    165.0|2019-10-10 13:42:26|BANOS138CAGOY_2_S...|     Normal|\n",
      "|      20191010|          1645|           29|            57.491383|     0.69687|         0.34843|              82.5|                    93.0|                    165.0|2019-10-10 16:45:48|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1645|         1942|            57.491383|     0.69687|         0.34843|              82.5|                    93.0|                    165.0|2019-10-10 16:45:48|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1829|           29|             71.89299|     0.87143|         0.43572|              82.5|                    93.0|                    165.0|2019-10-10 18:29:18|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1829|         1942|             71.89299|     0.87143|         0.43572|              82.5|                    93.0|                    165.0|2019-10-10 18:29:18|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1959|           29|             71.57125|     0.86753|         0.43377|              82.5|                    93.0|                    165.0|2019-10-10 19:59:59|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          1959|         1942|             71.57125|     0.86753|         0.43377|              82.5|                    93.0|                    165.0|2019-10-10 19:59:59|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          2122|           29|            62.902954|     0.76246|         0.38123|              82.5|                    93.0|                    165.0|2019-10-10 21:22:00|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          2122|         1942|            62.902954|     0.76246|         0.38123|              82.5|                    93.0|                    165.0|2019-10-10 21:22:00|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          2142|           29|            62.445583|     0.75692|         0.37846|              82.5|                    93.0|                    165.0|2019-10-10 21:42:16|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|          2142|         1942|            62.445583|     0.75692|         0.37846|              82.5|                    93.0|                    165.0|2019-10-10 21:42:16|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|           243|           29|            50.628876|     0.61368|         0.30684|              82.5|                    93.0|                    165.0|2019-10-10 02:43:43|C_AGOYAN138BANOS_...|     Normal|\n",
      "|      20191010|           243|         1942|            50.628876|     0.61368|         0.30684|              82.5|                    93.0|                    165.0|2019-10-10 02:43:43|C_AGOYAN138BANOS_...|     Normal|\n",
      "+--------------+--------------+-------------+---------------------+------------+----------------+------------------+------------------------+-------------------------+-------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fecha_inicio = datetime.datetime.strptime('2019-10-10 00:00', '%Y-%m-%d %H:%M')\n",
    "fecha_fin = datetime.datetime.strptime('2019-10-10 23:59', '%Y-%m-%d %H:%M')\n",
    "ejecucion = LimpiezaBL()\n",
    "ejecucion.Ejecucion(fecha_inicio,fecha_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbscan.Dibujar_Resultados(plt, circuito = circuito,tag = tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
